{"title":"KL Divergence: Differences Between Expected Differences","markdown":{"yaml":{"title":"KL Divergence: Differences Between Expected Differences"},"headingText":"Comparing Distributions: Relative Entropy & Redundancy","containsRefs":false,"markdown":"\n\n\n\n\n\n\nLet's reiterate what we've learned so far.\n\nFirst, we've learned to calculate:\n\n1. The bits to encode the outcomes of *one* distribution\n2. The average bits for this encoding\n\nSecond, we know that:\n\n1. The information entropy of *one* distribution consists of the weighted sum of bits.\n2. Entropy measures surprise. The more uniform, the higher the entropy. An entirely uniform distribution is the \"maximum entropy\" distribution.\n\n\nBut what if we want to compare distributions? What if we want to somehow measure the entropy of their difference?\n\nLet's start with two familiar examples. \n- We have our language with $A$, $B$, $C$ and $D$. \n- We know their probabilities. \n- We also know what it would look like if we assumed an uniform distribution for them. \n\nLet's now compare these two distributions.\n\nOne way we could compare them, is to just take their ratio. We will call this ratio **\"relative entropy\"**:\n\n$$\n\\text{Relative entropy} = \\frac{\\text{Entropy of a distribution}}{\\text{The max entropy of the same distribution}}\n$$\n\n**PROMPT:** Can you write that in Python for our example above? Wrap it in a function like this: ```def relative_entropy(true_ent, max_ent)```.\n\nRecall the entropies of these two distributions:\n\nShannon introduces this concept in his work to describe the difference between the entropy of our source and a completely uniform version of the same source. In Shannon's (p. 56) own words:\n\n> The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols will be called its *relative entropy*. \n\nWhat does relative entropy capture then? Weaver gets into this question in his intro to Shannon's work:\n\n> If the relative entropy of a certain source is, say .8, this roughly means that this source is, in its choice of symbols to form a message, about 80 per cent as free as it could possibly be with these same symbols. \n\nShannon himself did a bunch of experiments to conclude that the redundancy of written English is about 50%. In his opinion:\n\n> This means that when we write English half of what we write is determined by the structure of the language and half is chosen freely.\n\nOne way to better understand what relative entropy captures, is by looking at it's relationship to **redundancy**. Shannon defined **redundancy** in terms of relative entropy as:\n\n$$\n\\text{Redundancy }= 1-\\text{ Relative Entropy}\n$$\n\nRelative entropy tells us how much freedom we have and redundancy tells us how much we lack. The uniform distribution is total freedom: Everything is equally possible! Once we add structure, our choices become more constrained. According to Shannon, redundancy\n\n> is the fraction of the structure of the message which is determined not by the free choice of the sender, but rather by the accepted statistical rules governing the use of the symbols in question.\n\nThere are some interesting philosophical implications here: Our use of language is just the actualization of the virtual space of possible outcomes, governed by statistical rules. A decisively posthuman vision of communication!\n\n**PROMPT:** What is the redundancy of our artificial language?\n\nIf we go back to our actual data, we can see that our language has 0.875 relative entropy and 0.125 redundancy.\n\nWhile Shannon and Weaver limited their analysis of relative entropy to the ratio between the entropy of a source and its max entropy, the principle of calculating such ratios can be extended.\n\nLet's now instead say that we are familiar with the language $A$, $B$, $C$ $D$, but we don't quite know what the probabilities for each outcome are. Based on our prior knowledge, we would guess it is something like this:\n\n$$\n\\begin{aligned}\np(A) &= 0.625 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.125 \\\\\np(D) &= 0.125\n\\end{aligned}\n$$\n\nComparing these visually, we see that it's an okay estimation.\n\nThe entropy for our estimate is:\n\nAnd compared to you actual entropy, we see that our assumed language is actually more structured. The high probability given to $A$ makes it more deterministic:\n\nLet's stop for a moment to recall what we set out to do: We wanted to find a metric to measure the difference between different probability distributions. And now we've done?!\n\nWell yes, kind of. But there's a problem. \n\n**PROMPT:** What might isse be with using relative entropy to compare distributions?\n\nWell, the problem is that relative entropy doesn't account for the individual probabilities of our outcomes. What kind of issues might this create? Let's clarify with an example.\n\nImagine our language from above, but with inverse probabilities. So:\n\n$$\n\\begin{aligned}\np(A) &= 0.125 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.25  \\\\\np(D) &= 0.5\n\\end{aligned}\n$$\n\nNow we see that while our distributions have shaped that mirror each other, our estimate would surely produce terrible predictions!\n\n**PROMPT:** What is the relative entropy between these two distributions?\n\nRelative entropy says nothing about the actual shape of the distribution, it just tells what it's mean is. We will need sharper tools to actually compare distributions. But we will still use entropy!\n\n## Comparing Distributions: The Kullback-Leibler Divergence\n\nToday, no one uses relative entropy or redundancy in machine learning, at least not in the sense that they were defined by Shannon. Instead, people use metrics that were further developed from the idea of relative entropy by other people who built on Shannon's work.\n\nOne particularly important measure like this is the Kullback-Leibler Divergence or $\\mathbb{KL}$. \n- It was developed by mathemtaicians Solomon Kullback and Richard Leibler in a 1951 paper. \n- While they didn't name it after themselves, other people since then have started using this name. \n- It's from the $\\mathbb{KL}$ divergence that cross-entrop, the perhaps most commonly used tool for comparing distributions in ML, is derived.\n\nLet's start with intuition again.\n\nWhat if for every outcome $x_i$ in our distribution, we compared the number of bits we need to encode that outcome?\n\n**PROMPT:** Can you code such a for-loop in Python?\n\nWhat? We actually need in total one bit more to encode our structured language than we need for the uniform distribution. And indeed, this is true, we see it from our bit tables:\n\nOur structured language requires 9 bits to encode, our uniform distribution only takes 8:\n\nBut entropy isn't about sums, it's about **weighted sums**, also known as **\"expected value\"** or, simply, **\"mean\"**.\n\nSo if we rewrite our loop, but now weigh every difference. What should we weigh it by? \n- Well, how about the probabilities of the actual distribution we are interested in? \n- Weighing the loop with the probabilities of our language, we get:\n\nWhat we now get is the distance of the uniform distribution from the vantage point of our the probabilities of our artificial language. And this is the $\\mathbb{KL}$ divergence! That's all there is to it. Let's write it in numpy and try it out.\n\n**PROMPT:** Write the $\\mathbb{KL}$ divergence in ```numpy```.\n\n**Hint:** Here you have it as a loop:\n\nNice! Let's try it out:\n\nOur uniform distribution is on average 0.25 bits away from our actual distribution.\n\nWhy is this true? Because, as we saw above, their total distance is $1$, so with four outcomes the average distance is $\\frac{1}{4} = 0.25$.\n\nOur estimate for the language is already a lot closer:\n\nThen, if we compare our language to itself, the distance is $0$:\n\nThis means that we know exactly what distribution $P$ generated the data and the difference in bits needed to encode the distributions is zero. In other words, \n\n> it means that we can correctly predict the probabilities of all possible future events, and thus we have learned to predict the future as well as an ’oracle’ that has access to the true distribution P (Murphy 2021, 243).\n\nFinally, how does the $\\mathbb{KL}$ divergence do on our inverse distribution?\n\nQuite well, it turns out! Whereas relative entropy was unable to distinguish them, $\\mathbb{KL}$ divergence shows that they are further away from each other than any other distributions we compared.\n\n\\begin{aligned}\n\\mathbb{KL}(p \\Vert q)\n    &= \\overbrace{E\\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big]}^{\\color{red}{\\text{Expected surprise of p when encoding with q}}} \\\\\n    &= \\sum_i p(x_i) \\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big] \\\\\n    &= \\sum_i p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)} \\\\\n    &= \\sum_i p(x_i)\\log_2 p(x_i) - \\sum_i p(x_i)\\log_2 q(x_i) \\\\\n    &= \\underbrace{-\\mathbb{H}(p)}_{\\text{Negentropy of p}} + \\underbrace{\\mathbb{H}(p,q)}_{\\text{Cross-entropy between p and q}}\n\\end{aligned}\n\nLooks hard? Well, sure. But mostly because we aren't familiar with the notation and the associated rules of different symbols. Building up the intuition slowly and in code, it's hopefully more clear :)\n\nNow we can move on to our last part: **cross-entropy**. This function is the last terms of the $\\mathbb{KL}$ divergence above and probably the most commonly used optimization function in neural networks today.\n\n## Comparing Distributions: Cross-Entropy\n\nWith cross-entropy, we take a step back and go to the original definition of entropy. \n- What if we encoded our artificial language with bits corresponding to some other language. \n- For example: What if we used the bits for the uniform distribution to encode the artificial language? \n    - How many bits would we then need on average. \n\nLet's say the distributions $P$ and $Q$ have the same outcomes $x_i$, but with different probabilities $p(x_i)$ and $q(x_i)$. Then, in terms of our equation for information, it would look like this:\n\n$$\n\\begin{equation}\n  \\mathbb{H}(P,Q) = -\\overbrace{\\sum_{i=1}^N p(x_i)}^\\text{Weighted sum for P} \\underbrace{\\log_2 q(x_i)}_\\text{Bits for Q}\n\\end{equation}\n$$\n\nWe can modify our $\\mathbb{KL}$ divergence loop accordingly.\n\n**PROMPT:** Write cross-entropy with a for-loop or in ```numpy```. <div> **Hint:** You can use both the for-loop and ```numpy```implementations of $\\mathbb{KL}$ divergence and the equation above.\n\nWhat do these numbers tell us? Well, just how many bits we need on average to encode our artificial language if we instead assume it is uniform. For our estimate, it's already lower:\n\n\ncross_entropy_np(lang_probs, assumed_probs)\n\nBut the most efficient encoding is achieved using the actual distribution of the langauge itself. And, indeed, the cross-entropy between our language and itself is just the entropy of the language! Like plain old entropy, it tells us how many bits we need on average to code the language with it's own encoding.\n\nAnd that's it! You now know as much if not a lot more information theory as most people doing machine learning. Most people don't really understand cross-entropy, they just use it :-S\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\nLet's reiterate what we've learned so far.\n\nFirst, we've learned to calculate:\n\n1. The bits to encode the outcomes of *one* distribution\n2. The average bits for this encoding\n\nSecond, we know that:\n\n1. The information entropy of *one* distribution consists of the weighted sum of bits.\n2. Entropy measures surprise. The more uniform, the higher the entropy. An entirely uniform distribution is the \"maximum entropy\" distribution.\n\n## Comparing Distributions: Relative Entropy & Redundancy\n\nBut what if we want to compare distributions? What if we want to somehow measure the entropy of their difference?\n\nLet's start with two familiar examples. \n- We have our language with $A$, $B$, $C$ and $D$. \n- We know their probabilities. \n- We also know what it would look like if we assumed an uniform distribution for them. \n\nLet's now compare these two distributions.\n\nOne way we could compare them, is to just take their ratio. We will call this ratio **\"relative entropy\"**:\n\n$$\n\\text{Relative entropy} = \\frac{\\text{Entropy of a distribution}}{\\text{The max entropy of the same distribution}}\n$$\n\n**PROMPT:** Can you write that in Python for our example above? Wrap it in a function like this: ```def relative_entropy(true_ent, max_ent)```.\n\nRecall the entropies of these two distributions:\n\nShannon introduces this concept in his work to describe the difference between the entropy of our source and a completely uniform version of the same source. In Shannon's (p. 56) own words:\n\n> The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols will be called its *relative entropy*. \n\nWhat does relative entropy capture then? Weaver gets into this question in his intro to Shannon's work:\n\n> If the relative entropy of a certain source is, say .8, this roughly means that this source is, in its choice of symbols to form a message, about 80 per cent as free as it could possibly be with these same symbols. \n\nShannon himself did a bunch of experiments to conclude that the redundancy of written English is about 50%. In his opinion:\n\n> This means that when we write English half of what we write is determined by the structure of the language and half is chosen freely.\n\nOne way to better understand what relative entropy captures, is by looking at it's relationship to **redundancy**. Shannon defined **redundancy** in terms of relative entropy as:\n\n$$\n\\text{Redundancy }= 1-\\text{ Relative Entropy}\n$$\n\nRelative entropy tells us how much freedom we have and redundancy tells us how much we lack. The uniform distribution is total freedom: Everything is equally possible! Once we add structure, our choices become more constrained. According to Shannon, redundancy\n\n> is the fraction of the structure of the message which is determined not by the free choice of the sender, but rather by the accepted statistical rules governing the use of the symbols in question.\n\nThere are some interesting philosophical implications here: Our use of language is just the actualization of the virtual space of possible outcomes, governed by statistical rules. A decisively posthuman vision of communication!\n\n**PROMPT:** What is the redundancy of our artificial language?\n\nIf we go back to our actual data, we can see that our language has 0.875 relative entropy and 0.125 redundancy.\n\nWhile Shannon and Weaver limited their analysis of relative entropy to the ratio between the entropy of a source and its max entropy, the principle of calculating such ratios can be extended.\n\nLet's now instead say that we are familiar with the language $A$, $B$, $C$ $D$, but we don't quite know what the probabilities for each outcome are. Based on our prior knowledge, we would guess it is something like this:\n\n$$\n\\begin{aligned}\np(A) &= 0.625 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.125 \\\\\np(D) &= 0.125\n\\end{aligned}\n$$\n\nComparing these visually, we see that it's an okay estimation.\n\nThe entropy for our estimate is:\n\nAnd compared to you actual entropy, we see that our assumed language is actually more structured. The high probability given to $A$ makes it more deterministic:\n\nLet's stop for a moment to recall what we set out to do: We wanted to find a metric to measure the difference between different probability distributions. And now we've done?!\n\nWell yes, kind of. But there's a problem. \n\n**PROMPT:** What might isse be with using relative entropy to compare distributions?\n\nWell, the problem is that relative entropy doesn't account for the individual probabilities of our outcomes. What kind of issues might this create? Let's clarify with an example.\n\nImagine our language from above, but with inverse probabilities. So:\n\n$$\n\\begin{aligned}\np(A) &= 0.125 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.25  \\\\\np(D) &= 0.5\n\\end{aligned}\n$$\n\nNow we see that while our distributions have shaped that mirror each other, our estimate would surely produce terrible predictions!\n\n**PROMPT:** What is the relative entropy between these two distributions?\n\nRelative entropy says nothing about the actual shape of the distribution, it just tells what it's mean is. We will need sharper tools to actually compare distributions. But we will still use entropy!\n\n## Comparing Distributions: The Kullback-Leibler Divergence\n\nToday, no one uses relative entropy or redundancy in machine learning, at least not in the sense that they were defined by Shannon. Instead, people use metrics that were further developed from the idea of relative entropy by other people who built on Shannon's work.\n\nOne particularly important measure like this is the Kullback-Leibler Divergence or $\\mathbb{KL}$. \n- It was developed by mathemtaicians Solomon Kullback and Richard Leibler in a 1951 paper. \n- While they didn't name it after themselves, other people since then have started using this name. \n- It's from the $\\mathbb{KL}$ divergence that cross-entrop, the perhaps most commonly used tool for comparing distributions in ML, is derived.\n\nLet's start with intuition again.\n\nWhat if for every outcome $x_i$ in our distribution, we compared the number of bits we need to encode that outcome?\n\n**PROMPT:** Can you code such a for-loop in Python?\n\nWhat? We actually need in total one bit more to encode our structured language than we need for the uniform distribution. And indeed, this is true, we see it from our bit tables:\n\nOur structured language requires 9 bits to encode, our uniform distribution only takes 8:\n\nBut entropy isn't about sums, it's about **weighted sums**, also known as **\"expected value\"** or, simply, **\"mean\"**.\n\nSo if we rewrite our loop, but now weigh every difference. What should we weigh it by? \n- Well, how about the probabilities of the actual distribution we are interested in? \n- Weighing the loop with the probabilities of our language, we get:\n\nWhat we now get is the distance of the uniform distribution from the vantage point of our the probabilities of our artificial language. And this is the $\\mathbb{KL}$ divergence! That's all there is to it. Let's write it in numpy and try it out.\n\n**PROMPT:** Write the $\\mathbb{KL}$ divergence in ```numpy```.\n\n**Hint:** Here you have it as a loop:\n\nNice! Let's try it out:\n\nOur uniform distribution is on average 0.25 bits away from our actual distribution.\n\nWhy is this true? Because, as we saw above, their total distance is $1$, so with four outcomes the average distance is $\\frac{1}{4} = 0.25$.\n\nOur estimate for the language is already a lot closer:\n\nThen, if we compare our language to itself, the distance is $0$:\n\nThis means that we know exactly what distribution $P$ generated the data and the difference in bits needed to encode the distributions is zero. In other words, \n\n> it means that we can correctly predict the probabilities of all possible future events, and thus we have learned to predict the future as well as an ’oracle’ that has access to the true distribution P (Murphy 2021, 243).\n\nFinally, how does the $\\mathbb{KL}$ divergence do on our inverse distribution?\n\nQuite well, it turns out! Whereas relative entropy was unable to distinguish them, $\\mathbb{KL}$ divergence shows that they are further away from each other than any other distributions we compared.\n\n\\begin{aligned}\n\\mathbb{KL}(p \\Vert q)\n    &= \\overbrace{E\\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big]}^{\\color{red}{\\text{Expected surprise of p when encoding with q}}} \\\\\n    &= \\sum_i p(x_i) \\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big] \\\\\n    &= \\sum_i p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)} \\\\\n    &= \\sum_i p(x_i)\\log_2 p(x_i) - \\sum_i p(x_i)\\log_2 q(x_i) \\\\\n    &= \\underbrace{-\\mathbb{H}(p)}_{\\text{Negentropy of p}} + \\underbrace{\\mathbb{H}(p,q)}_{\\text{Cross-entropy between p and q}}\n\\end{aligned}\n\nLooks hard? Well, sure. But mostly because we aren't familiar with the notation and the associated rules of different symbols. Building up the intuition slowly and in code, it's hopefully more clear :)\n\nNow we can move on to our last part: **cross-entropy**. This function is the last terms of the $\\mathbb{KL}$ divergence above and probably the most commonly used optimization function in neural networks today.\n\n## Comparing Distributions: Cross-Entropy\n\nWith cross-entropy, we take a step back and go to the original definition of entropy. \n- What if we encoded our artificial language with bits corresponding to some other language. \n- For example: What if we used the bits for the uniform distribution to encode the artificial language? \n    - How many bits would we then need on average. \n\nLet's say the distributions $P$ and $Q$ have the same outcomes $x_i$, but with different probabilities $p(x_i)$ and $q(x_i)$. Then, in terms of our equation for information, it would look like this:\n\n$$\n\\begin{equation}\n  \\mathbb{H}(P,Q) = -\\overbrace{\\sum_{i=1}^N p(x_i)}^\\text{Weighted sum for P} \\underbrace{\\log_2 q(x_i)}_\\text{Bits for Q}\n\\end{equation}\n$$\n\nWe can modify our $\\mathbb{KL}$ divergence loop accordingly.\n\n**PROMPT:** Write cross-entropy with a for-loop or in ```numpy```. <div> **Hint:** You can use both the for-loop and ```numpy```implementations of $\\mathbb{KL}$ divergence and the equation above.\n\nWhat do these numbers tell us? Well, just how many bits we need on average to encode our artificial language if we instead assume it is uniform. For our estimate, it's already lower:\n\n\ncross_entropy_np(lang_probs, assumed_probs)\n\nBut the most efficient encoding is achieved using the actual distribution of the langauge itself. And, indeed, the cross-entropy between our language and itself is just the entropy of the language! Like plain old entropy, it tells us how many bits we need on average to code the language with it's own encoding.\n\nAnd that's it! You now know as much if not a lot more information theory as most people doing machine learning. Most people don't really understand cross-entropy, they just use it :-S\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"include-in-header":["../hide-html-code.html"],"toc":true,"toc-depth":4,"output-file":"03_kl_divergence.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","monofont":"JetBrains Mono","theme":"cosmo","toc-location":"left","math":"mathjax","code-summary":"Code Toggle","title":"KL Divergence: Differences Between Expected Differences"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}