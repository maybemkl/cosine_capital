{"title":"Bits: The Difference that Makes a Difference","markdown":{"yaml":{"title":"Bits: The Difference that Makes a Difference"},"headingText":"The General Commmunications System","containsRefs":false,"markdown":"\n\n\n\n\n\n\n\nRecall from the introdution the \"conduit metaphor\" that guides not just information theory but more broadly many of our received notions of communications today. As I wrote, this metaphor is most neatly encapsulated in the \"General Communication System\" deviced by Claude Shannon in a seminal 1948 paper. Shannon envisioned communication as a system with distinct analytical parts, which you can explore in this interactive figure:\n\n::: {.column-margin}\nShannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379–423. The Bell System Technical Journal. [[URL]](https://ieeexplore.ieee.org/document/6773024)\n::: \n\nWhat we have here, first, is an **information source** that selects a message out of a finite set of possible messages. For instance, this might be a person typing on a keyboard. In fact, the message could come from *any* stochastic and ergodic process, which simply means a random process which, over time, is regular enough to form statistical patterns. \n\nNext, these messages are encoded by a **transmitter** which converts them into a code. We saw the example of Morse in the introduction. However, here we will be looking at the encoding of messages as the *bits*, zeroes and ones. When typing on a keyboard, our computer does this for us. \n\nOnce the message is encoded as bits, it can be sent over as a **signal** over a **channel**. Effectively, the \"channel\" is all the material infrastructdure between a sender and a receiver, including in our example fiberoptic cables, routers, internet protocols, etc. In this process, some **noise** might be introduced to the signal. For instance, maybe some bits are scrambled when a data package is lost or perhaps a malicious third party modifies the signal.\n\nBe that as it may, we hopefully reach a **receiver** that decodes the message, usually by running the inverse operation of the transmitter. In our example, bits are turned into words and the words reach their **destination**, which Weaver and Shannon (1963, 33—34, 57) defined as \"[t]he person or thing for whom the message is intended.\"\n\n::: {.column-margin}\nShannon, C. E., & Weaver, W. (1963). The Mathematical Theory of Communication (1st paperback). University of Illinois Press.\n:::\n\nIn this chapter, we will start from the encoding of words as bits in the transmitter. Why bits? And what do we have to assume of language to encode it like that? \n\n## The Information Source: Stochastic Parrots\n\nInformation theory as we know it starts from cryptographic work done during World War II by Shannon and others. Whereas the early information theory of the interwar era by people such as Ralph Hartley and Harry Nyquist treated language as a sort of constant to be dealt with in the domain of telephony and telegraphy, Shannon took a different approach. Leaning on insights by earier cryptographers, Shannon approach language as  \"stochastic\" and \"ergodic\" process. What this suggests is simply that there is statistical regularity to how frequent words are in text.\n\n::: {.column-margin}\n> A language is considered for cryptographic purposes to be a stochastic process which produces a discrete sequence of symbols in accordance with some systems of probabilities.\n\nShannon, C. E. (1945). A Mathematical Theory of Cryptography—Case 10878. Bell Telephone Laboratories, Princeton Libraries. (p. 2)\n:::\n\nWhat this implies is a strong hypothesis about how language works and about the **source** that produces the inputs in the General System of Communication. Rather than people choosing words to speak at will, language itself appears as a constraint to which we have to adapt as we speak. Indeed, it is almost as if it was language that used us, rather than the other way around. Before we dive into the world of bits, we need to dwell on this conceptualization of the source that produces the inputs to its systems. This section is dedicated to that task.\n\n::: {.column-margin}\nGeoghegan, B. D. (2019). Architectures of information—A comparison of Wiener’s and Shannon’s theories of information. In T. Vardouli & O. Touloumi (Eds.), *Computer Architectures: Constructing the Common Ground*. Routledge. [[URL]](https://www.taylorfrancis.com/chapters/edit/10.4324/9780429264306-8/architectures-information-bernard-dionysius-geoghegan)\n:::\n\nIn a famous critique of LLMs, Emily Bender and co-authors described these and other language models (LMs) as a \"system for haphazardly stitching together sequences of linguistic form\":\n\n::: {.column-margin} \nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &#x1f99c; Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–623. [[URL]](https://doi.org/10.1145/3442188.3445922)\n:::\n\n> Contrary to how it may seem when we observe its output, an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning a stochastic parrot.\n\nWhat does it mean? Simply that LMs draw on observed probabilities of words and their co-occurence to form the sentences that we see on services like ChatGPT and Gemini. They treat language as a **probability distribution**. While LLMs find complex dependencies in how words co-occur over long passages of text, we will start with a much simpler representation of language.\n\n### Zipf's Law\n\nThis idea of treating language probabilistically is very old. Already in 1932, the linguist George Zipf had observed that word frequency is inversely proportional to work rank, a regularity which later became known as [\"Zipf's law\"](https://en.wikipedia.org/wiki/Zipf%27s_law). Zipf's Law is an example of one of the most simple language models, an early representation of language as a stochastic process. Zipf's Law is interesting, because it is based on *observed* frequencies of words. In this sense, it is a herald of the empirical approach to language that predominates in language modelling today. Using these observed probabilities of individual words, we can already build a simple language model. Let us do just that using Lewis Carroll's \"Alice in Wonderland\". For this purpose, we will use the [NLTK](https://www.nltk.org/) library to download the text and to \"tokenize\" it into individual words.\n\nUsing a custom function we can see how this tokenization works:\n\nWe can also count the words, to get an initial sense of what kind of distribution to expect:\n\n### A simple language model\n\nWhat do these counts mean in terms of probabilities? To estimate that, we count how many times each token appears (its frequency) and then divide that by the total number of tokens.\n\nThe top 10 most common words in Alice in Wonderland are mostly so called \"stopwords\" (the, and, to, a, etc.) and words relating to dialogue:\n\nTogether, the probabilities of these words form a **probability distribution**. For the 15 most common words, the distribution looks like this:\n\nOnce we have a distribution of observed probabilities, we can also *draw* from it. This means that we sample from our representation of language to produce a sequence of words. This is, effectively, what large language models also do, but with a lot more complicated representations. Nevertheless, what we have at hand is a *de facto* language model.\n\nTo draw a fifty word sentence from our representation of language as it appears in Alice in Wonderland, we can run the following code:\n\nNot a great representation! But it's also not entirely random. The word \"the\" appears over and over again, simply because it appears the most often in this (and any given English) text. However, we could work with a completely random sample as well. In this case, we would just give each word the same probability.\n\nNo the word \"the\" doesn't appear a single time! Visually, the probability distribution now looks like this for the first 18 words:\n\nUnlike our representation using observed word probabilities, we use a simplifying assumption here. We loose all sense of structure, everything becomes uniform. By contrast, the observed probabilities already had some **redundancy**. We know, for instance, that by always picking the word \"the\" we will be more likely to correctly predict a draw from the text than always picking the word \"alice\". \n\n::: {.column-margin} \n> The redundancy, on the other hand, measures the amount of constraint imposed on a text in the language due to its statistical structure, e.g., in English the high frequency of the letter E, the strong tendency of H to follow T or of L' to follow Q.\n\nShannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. [[URL]](https://archive.org/details/bstj30-1-50)\n:::\n\n## The Transmitter: Words as Bits\n\nOkay, now we've established some key things:\n- We know what a distribution is\n- We know that Shannon and other pioneers of cybernetics thought of language as a stochastic process applying to discrete symbols and words\n    - Ergo: Language was a distribution for them \n- If we know some of the statistical regularities or the \"redundancies\" of this distribution we can work on problems like cryptography and machine translation.\n    - Zipf's Law is an example of a very simple approximation of such redundancies, based solely on a generalization from observed word frequencies.\n\nHowever, in order to do anything practical with this hypothesis about the **source**, our distributions of interested have to be somehow represented and encoded in order to be **transmitted**. Let's revisit the image of the noisy channel. In Morse code, we would encode our words using the long and short codes, encoded as such by a [telegraph key](https://en.wikipedia.org/wiki/Telegraph_key). This would be our **transmitter**. However, Morse is not the most efficient way to represent language.\n\nBasing his ideas on innovations in \"Boolean algebra\", Shannon decided that the most intuitive way to represent anything was the most simple one: as \"yes\" or \"no\", 0 or 1. A poll in the coffee room of Bell Labs–where Shannon worked at the time-established that this unit should be known as a \"bit\" of information.\n\n::: {.column-margin} \nShannon, C. E. (1940). A symbolic analysis of relay and switching circuits [Thesis, Massachusetts Institute of Technology]. [[URL]](https://dspace.mit.edu/handle/1721.1/11173)\n:::\n\n### The most simple language\n\nIn his original papers, Shannon demonstrated the first principles of information theory using a coin toss, only then moving on to human language. We will follow Shannon there. In fact, a coin toss might be one of the most simple stochastic processes, as it only has two outcomes (heads or tails) and, if the coin is fair, no structure or redundancy.\n\nIn order to work with the probabilities of a coin toss in Python, we are going to use `numpy` to generate probability distributions and show the behavior of coins that are fair (heads and tails are equally likely) and biased (heads is more likely than tails, or vice versa.).\n\nUsing the code below, we can generate the probabilities of a coin that is biased towards tails (the first five values) or heads (the last five values).\n\nAbove, we let the probability of heads increase in small increments from 0 to 1. That gives us a list of different values for $P(H)$, for different \"coins\" with different biases, if you will. Now we can get a similar sized but *inverse* list of $P(T)$ by just taking $1 - P(H)$.\n\n**PROMPT:** Generate ```p_heads``` using ```linspace``` and then use ```p_heads``` to create ```p_tails```.\n\nPlotting the probabilities of heads and tails for each of our nine coins, we can see how the probability of one falls as the probability for the other goes up.\n\nThis might seem stupidly self-evident, but there is a point here: We are establishing a very simple baseline from which to build up our understanding of bits and information. In fact, how could we represent the possible outcomes of a *single* and *fair* coin toss as bits?\n\n**PROMPT:** If we want to represent a single, fair coin toss as bits, how do we proceed? How many bits do we need in total?\n\nAs you can see, we can represent heads as 0 and tails as 1. And, in total, we need two bits to represent the *sample space* of a single toss of a fair coin. Using notation from probability and set theory, we might write:\n\n$A = \\{H, T\\}$\n\nOur sample space consists of just two options, and we only need two bits to represent it (assuming each outcome is equally likely!).\n\n<details>\n<summary><strong>PROMPT:</strong> Let's move up. What is the sample space of two successive coin tosses of a fair coin?</summary><br>\n\n$A = \\{HH, HT, TT, TH \\}$\n</details>\n\n**PROMPT:** If we want to represent this sample space of two coin tosses in bits, how do we do it?\n\n<details>\n<summary><strong>PROMPT:</strong> So far, we have been counting <i>total</i> bits. However, how many questions would we need <i>on average</i> to find out what the outcome was in the last experiment?</summary><br>\nTwo questions!\n    \n**First question:** Is the first throw heads? If yes, we have $\\{HH, HT\\}$ left. If no, we have $\\{TT, TH \\}$ left. \n\n**Second question:** Is the first throw heads? Whatever the answer, only one option will remain. \n\nOf course we could get lucky by asking just one question. Say, for example, the outcome was $HH$. If we ask \"was it heads and heads\", we would've needed only one question. But then we would've been lucky. *On average* we need two questions.\n</details>\n\n\n\n::: {.column-margin}\nBateson, G. (2000). \"Double Bind\" in *Steps to an Ecology of Mind: Collected Essays in Anthropology, Psychiatry, Evolution, and Epistemology*. University of Chicago Press.\n:::\nThis was the big innovation from Shannon. A **bit is the number of yes-no questions we need to ask in order to know an outcome in an experiment**. This is why Gregory Bateson called a bit \"the difference which makes a difference.\"  If we start with the bit $0$, the $0$ is the first \"difference\" which makes the difference that our sequence is one that starts with heads. If we add a $0$ to the bit sequence, the \"difference\" which is made determines that the full sequence is $HH$. Once we get to represent language with bits in the next section, this becomes less tautological.\n\nWhat is more, this average number of bits that we need to answer as yes-no question was, in fact, how Shannon **defined information**. Before we go on to formalize this discovery mathematically, let's build up some more intuition.\n\nLet's consider an experiment with three trials or tosses. Typing that out is annoying, so we'll use a helper function for it.\n\n**PROMPT:** How big is the sample space of eight trials? You can use ```produce_N_compitations``` to find out.\n\n**PROMPT:** How would we encode that in bits? Use ```produce_N_combinations``` again, if you want.\n\n<details>\n<summary><strong>PROMPT:</strong> How many questions do we need now?</summary><br>\n\nThree questions!\n\n**First question:** Is the first throw heads? If yes, we have $\\{HTT, HTH, HHT, HHH\\}$ left. If no, we have $\\{TTT, TTH, THT, THH \\}$ left. \n\nWith four options left, we know from above we need just two more questions. So three questions in total!\n</details>\n\n<details>\n<summary><b>PROMPT:</b> Do you notice some sort of connection between how we build our outcomes and the number of bits we need to describe the distribution?</summary>\n\nYes, there is indeed a connection. And that’s good, because all this counting can get tiresome…\n\nTo know how many options we have to choose among, we can use a simple formula. If $S$ is the number of symbols we can choose from (i.e. 2, because we choose between $H$ and $T$) and $n$ is the number of combinations of those (3 in the above example), then the number of basic outcomes $E$ (e.g. $HH$, $HT$) is:\n\n$E = S^n$\n</details>\n\nLet's try it out:\n\nTwo outcomes $H$ and $T$ can be combined combined in sequences of three in eight different ways, just as we saw above.\n\n**PROMPT:** How many bits will we need to represent these options?\n\nIf we already know $E$, we can find out how many bits we need by taking the *logarithm* of $E$, because the logarithm is simply the inverse of a power:\n\nIn fact, we can count the number of bits we need directly from $n$ and $S$:\n\nThis is also how Ralph Hartley, an engineer at Bell Labs, defined information in a paper in 1928. For him information was just:\n\n::: {.column-margin} \nHartley, R. V. L. (1928). Transmission of Information. Bell System Technical Journal, 7(3), 535–563. [[URL]](https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1928.tb01236.x)\n<br>\n<br>\nNyquist, H. (1924). Certain factors affecting telegraph speed. The Bell System Technical Journal, 3(2), 324–346. The Bell System Technical Journal. [[URL]](https://doi.org/10.1002/j.1538-7305.1924.tb01361.x)\n\n\n:::\n\n$$\nn \\cdot \\log S\n$$\n\nBut this isn't the definition of that history settled on, and *only* applies to uniform distributions where all outcomes are equally likely. In fact, already before Shannon, Hartley and Ralph Nyquist at Bell Labs had defined \"intelligence\" in the above manner. While entirely satisfactory for uniform distributions, this definition does not work when we stop assuming all outcomes are equally probable. Furthermore, we have not even worked with actual probabilities yet! Instead, we have just assumed uniformity. In order to move away from that assumption, we have to define bits for probabilities.\n\nBefore we move on to deal with that in final part of this section, let's end this one by taking a little detour to understand the logarithm. If you are already familiar with logs, feels free to jump to the last part on bits for probabilities.\n\n#### Aside: What Does the Logarithm Do?\n\nNotice how we used `numpy.log2` in the previous section to calculate the number of bits needed to encode our sequence of coin tosses. In, fact, we use $log_2$ because it assumes $S=2$, 0 or 1, \"yes\" or \"no\". If we use some other logarithm, it's no longer the number of bits we are calculating. For instance, using a logarithm of the natural number $e$ would give us the necessary number of \"nats\".\n\n::: {.column-margin} \nFor a very approachable introduction to bits, including intuitive explanations of the logarithm, and information theory more broadly, I recommend the work of information theorist John Pierce.\n<br>\n<br>\nPierce, J. R. (1980). *An Introduction to Information Theory: Symbols, Signals and Noise*. (Second revised edition). Dover Books.\n::: \n\nThe logarithm to the base 2 of a number is the power to which 2 must be raised to equal the number. We can express the same thing mathematically:\n\n$$\n2^{\\log_2 x} = x\\\\\n$$\n\nWith our three tosses of heads and tails:\n\n$$\n\\log_2 8 = 3 \\\\\n2^{\\log_2 8} = 8 \\\\\n2^3 = 8 \n$$\n\nTwo must be raised to $3$ to give us 8.\n\n**PROMPT:** What are the logarithms of the following numbers: 1, 2, 4, 8, 16, 32, 64?\n\n| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n|---------------|---------------------|------------|\n| $1$           | $0$                 |$2^0 = 1$   |\n| $2$           | $1$                 |$2^1 = 2$   |\n| $4$           | $2$                 |$2^2 = 4$   |\n| $8$           | $3$                 |$2^3 = 8$   |\n| $16$          | $4$                 |$2^4 = 16$  |\n| $32$          | $5$                 |$2^5 = 32$  |\n| $64$          | $6$                 |$2^6 = 64$  |\n\nAs we can see when plotting the logarithms, the logarithm of a number $x$ first grows very quickly, but then slows down.\n\n### Bits for Probabilities\n\nSo far we have worked with *counts*, but we started out asking how we could encode *probabilities*. How could we make that leap? An interesting property with the logarithm, is that taking the log of a fraction gives us the same result as taking the log of the denominator *but negative*. So\n\n$$\n\\log_2 2 = 1\n$$\n\nAnd:\n\n$$\n\\log_2\\frac{1}{2} = - 1\n$$\n\nThis means that counting logs for a uniform distribution where all events are equally probable is just like working with counts, but negative!\n\n**PROMPT:** What are the logarithms of the following numbers: $\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}$\n\n| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n|---------------|---------------------|------------|\n| $1$           | $0$                 |$2^0 = 1$   |\n| $\\frac{1}{2}$           | $-1$                 |$2^{-1} = \\frac{1}{2}$   |\n| $\\frac{1}{4}$           | $-2$                 |$2^{-2} = \\frac{1}{4}$   |\n| $\\frac{1}{8}$           | $-3$                 |$2^{-3} = \\frac{1}{8}$   |\n| $\\frac{1}{16}$          | $-4$                 |$2^{-4} = \\frac{1}{16}$  |\n| $\\frac{1}{32}$          | $-5$                 |$2^{-5} = \\frac{1}{32}$  |\n| $\\frac{1}{64}$          | $-6$                 |$2^{-6} = \\frac{1}{64}$  |\n\nPlotting the log for our fractional $x$ values, we can note that the log grows very quickly as we start from very small values and move to larger ones, only to flatten out asymptotically when we approach $x$.\n\nOkay, given that we can use logarithms to count bits for probabilities, we are ready to move on to a more general definition of information that does not assume uniformity.\n\n\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\n## The General Commmunications System\n\nRecall from the introdution the \"conduit metaphor\" that guides not just information theory but more broadly many of our received notions of communications today. As I wrote, this metaphor is most neatly encapsulated in the \"General Communication System\" deviced by Claude Shannon in a seminal 1948 paper. Shannon envisioned communication as a system with distinct analytical parts, which you can explore in this interactive figure:\n\n::: {.column-margin}\nShannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379–423. The Bell System Technical Journal. [[URL]](https://ieeexplore.ieee.org/document/6773024)\n::: \n\nWhat we have here, first, is an **information source** that selects a message out of a finite set of possible messages. For instance, this might be a person typing on a keyboard. In fact, the message could come from *any* stochastic and ergodic process, which simply means a random process which, over time, is regular enough to form statistical patterns. \n\nNext, these messages are encoded by a **transmitter** which converts them into a code. We saw the example of Morse in the introduction. However, here we will be looking at the encoding of messages as the *bits*, zeroes and ones. When typing on a keyboard, our computer does this for us. \n\nOnce the message is encoded as bits, it can be sent over as a **signal** over a **channel**. Effectively, the \"channel\" is all the material infrastructdure between a sender and a receiver, including in our example fiberoptic cables, routers, internet protocols, etc. In this process, some **noise** might be introduced to the signal. For instance, maybe some bits are scrambled when a data package is lost or perhaps a malicious third party modifies the signal.\n\nBe that as it may, we hopefully reach a **receiver** that decodes the message, usually by running the inverse operation of the transmitter. In our example, bits are turned into words and the words reach their **destination**, which Weaver and Shannon (1963, 33—34, 57) defined as \"[t]he person or thing for whom the message is intended.\"\n\n::: {.column-margin}\nShannon, C. E., & Weaver, W. (1963). The Mathematical Theory of Communication (1st paperback). University of Illinois Press.\n:::\n\nIn this chapter, we will start from the encoding of words as bits in the transmitter. Why bits? And what do we have to assume of language to encode it like that? \n\n## The Information Source: Stochastic Parrots\n\nInformation theory as we know it starts from cryptographic work done during World War II by Shannon and others. Whereas the early information theory of the interwar era by people such as Ralph Hartley and Harry Nyquist treated language as a sort of constant to be dealt with in the domain of telephony and telegraphy, Shannon took a different approach. Leaning on insights by earier cryptographers, Shannon approach language as  \"stochastic\" and \"ergodic\" process. What this suggests is simply that there is statistical regularity to how frequent words are in text.\n\n::: {.column-margin}\n> A language is considered for cryptographic purposes to be a stochastic process which produces a discrete sequence of symbols in accordance with some systems of probabilities.\n\nShannon, C. E. (1945). A Mathematical Theory of Cryptography—Case 10878. Bell Telephone Laboratories, Princeton Libraries. (p. 2)\n:::\n\nWhat this implies is a strong hypothesis about how language works and about the **source** that produces the inputs in the General System of Communication. Rather than people choosing words to speak at will, language itself appears as a constraint to which we have to adapt as we speak. Indeed, it is almost as if it was language that used us, rather than the other way around. Before we dive into the world of bits, we need to dwell on this conceptualization of the source that produces the inputs to its systems. This section is dedicated to that task.\n\n::: {.column-margin}\nGeoghegan, B. D. (2019). Architectures of information—A comparison of Wiener’s and Shannon’s theories of information. In T. Vardouli & O. Touloumi (Eds.), *Computer Architectures: Constructing the Common Ground*. Routledge. [[URL]](https://www.taylorfrancis.com/chapters/edit/10.4324/9780429264306-8/architectures-information-bernard-dionysius-geoghegan)\n:::\n\nIn a famous critique of LLMs, Emily Bender and co-authors described these and other language models (LMs) as a \"system for haphazardly stitching together sequences of linguistic form\":\n\n::: {.column-margin} \nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? &#x1f99c; Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–623. [[URL]](https://doi.org/10.1145/3442188.3445922)\n:::\n\n> Contrary to how it may seem when we observe its output, an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning a stochastic parrot.\n\nWhat does it mean? Simply that LMs draw on observed probabilities of words and their co-occurence to form the sentences that we see on services like ChatGPT and Gemini. They treat language as a **probability distribution**. While LLMs find complex dependencies in how words co-occur over long passages of text, we will start with a much simpler representation of language.\n\n### Zipf's Law\n\nThis idea of treating language probabilistically is very old. Already in 1932, the linguist George Zipf had observed that word frequency is inversely proportional to work rank, a regularity which later became known as [\"Zipf's law\"](https://en.wikipedia.org/wiki/Zipf%27s_law). Zipf's Law is an example of one of the most simple language models, an early representation of language as a stochastic process. Zipf's Law is interesting, because it is based on *observed* frequencies of words. In this sense, it is a herald of the empirical approach to language that predominates in language modelling today. Using these observed probabilities of individual words, we can already build a simple language model. Let us do just that using Lewis Carroll's \"Alice in Wonderland\". For this purpose, we will use the [NLTK](https://www.nltk.org/) library to download the text and to \"tokenize\" it into individual words.\n\nUsing a custom function we can see how this tokenization works:\n\nWe can also count the words, to get an initial sense of what kind of distribution to expect:\n\n### A simple language model\n\nWhat do these counts mean in terms of probabilities? To estimate that, we count how many times each token appears (its frequency) and then divide that by the total number of tokens.\n\nThe top 10 most common words in Alice in Wonderland are mostly so called \"stopwords\" (the, and, to, a, etc.) and words relating to dialogue:\n\nTogether, the probabilities of these words form a **probability distribution**. For the 15 most common words, the distribution looks like this:\n\nOnce we have a distribution of observed probabilities, we can also *draw* from it. This means that we sample from our representation of language to produce a sequence of words. This is, effectively, what large language models also do, but with a lot more complicated representations. Nevertheless, what we have at hand is a *de facto* language model.\n\nTo draw a fifty word sentence from our representation of language as it appears in Alice in Wonderland, we can run the following code:\n\nNot a great representation! But it's also not entirely random. The word \"the\" appears over and over again, simply because it appears the most often in this (and any given English) text. However, we could work with a completely random sample as well. In this case, we would just give each word the same probability.\n\nNo the word \"the\" doesn't appear a single time! Visually, the probability distribution now looks like this for the first 18 words:\n\nUnlike our representation using observed word probabilities, we use a simplifying assumption here. We loose all sense of structure, everything becomes uniform. By contrast, the observed probabilities already had some **redundancy**. We know, for instance, that by always picking the word \"the\" we will be more likely to correctly predict a draw from the text than always picking the word \"alice\". \n\n::: {.column-margin} \n> The redundancy, on the other hand, measures the amount of constraint imposed on a text in the language due to its statistical structure, e.g., in English the high frequency of the letter E, the strong tendency of H to follow T or of L' to follow Q.\n\nShannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50–64. [[URL]](https://archive.org/details/bstj30-1-50)\n:::\n\n## The Transmitter: Words as Bits\n\nOkay, now we've established some key things:\n- We know what a distribution is\n- We know that Shannon and other pioneers of cybernetics thought of language as a stochastic process applying to discrete symbols and words\n    - Ergo: Language was a distribution for them \n- If we know some of the statistical regularities or the \"redundancies\" of this distribution we can work on problems like cryptography and machine translation.\n    - Zipf's Law is an example of a very simple approximation of such redundancies, based solely on a generalization from observed word frequencies.\n\nHowever, in order to do anything practical with this hypothesis about the **source**, our distributions of interested have to be somehow represented and encoded in order to be **transmitted**. Let's revisit the image of the noisy channel. In Morse code, we would encode our words using the long and short codes, encoded as such by a [telegraph key](https://en.wikipedia.org/wiki/Telegraph_key). This would be our **transmitter**. However, Morse is not the most efficient way to represent language.\n\nBasing his ideas on innovations in \"Boolean algebra\", Shannon decided that the most intuitive way to represent anything was the most simple one: as \"yes\" or \"no\", 0 or 1. A poll in the coffee room of Bell Labs–where Shannon worked at the time-established that this unit should be known as a \"bit\" of information.\n\n::: {.column-margin} \nShannon, C. E. (1940). A symbolic analysis of relay and switching circuits [Thesis, Massachusetts Institute of Technology]. [[URL]](https://dspace.mit.edu/handle/1721.1/11173)\n:::\n\n### The most simple language\n\nIn his original papers, Shannon demonstrated the first principles of information theory using a coin toss, only then moving on to human language. We will follow Shannon there. In fact, a coin toss might be one of the most simple stochastic processes, as it only has two outcomes (heads or tails) and, if the coin is fair, no structure or redundancy.\n\nIn order to work with the probabilities of a coin toss in Python, we are going to use `numpy` to generate probability distributions and show the behavior of coins that are fair (heads and tails are equally likely) and biased (heads is more likely than tails, or vice versa.).\n\nUsing the code below, we can generate the probabilities of a coin that is biased towards tails (the first five values) or heads (the last five values).\n\nAbove, we let the probability of heads increase in small increments from 0 to 1. That gives us a list of different values for $P(H)$, for different \"coins\" with different biases, if you will. Now we can get a similar sized but *inverse* list of $P(T)$ by just taking $1 - P(H)$.\n\n**PROMPT:** Generate ```p_heads``` using ```linspace``` and then use ```p_heads``` to create ```p_tails```.\n\nPlotting the probabilities of heads and tails for each of our nine coins, we can see how the probability of one falls as the probability for the other goes up.\n\nThis might seem stupidly self-evident, but there is a point here: We are establishing a very simple baseline from which to build up our understanding of bits and information. In fact, how could we represent the possible outcomes of a *single* and *fair* coin toss as bits?\n\n**PROMPT:** If we want to represent a single, fair coin toss as bits, how do we proceed? How many bits do we need in total?\n\nAs you can see, we can represent heads as 0 and tails as 1. And, in total, we need two bits to represent the *sample space* of a single toss of a fair coin. Using notation from probability and set theory, we might write:\n\n$A = \\{H, T\\}$\n\nOur sample space consists of just two options, and we only need two bits to represent it (assuming each outcome is equally likely!).\n\n<details>\n<summary><strong>PROMPT:</strong> Let's move up. What is the sample space of two successive coin tosses of a fair coin?</summary><br>\n\n$A = \\{HH, HT, TT, TH \\}$\n</details>\n\n**PROMPT:** If we want to represent this sample space of two coin tosses in bits, how do we do it?\n\n<details>\n<summary><strong>PROMPT:</strong> So far, we have been counting <i>total</i> bits. However, how many questions would we need <i>on average</i> to find out what the outcome was in the last experiment?</summary><br>\nTwo questions!\n    \n**First question:** Is the first throw heads? If yes, we have $\\{HH, HT\\}$ left. If no, we have $\\{TT, TH \\}$ left. \n\n**Second question:** Is the first throw heads? Whatever the answer, only one option will remain. \n\nOf course we could get lucky by asking just one question. Say, for example, the outcome was $HH$. If we ask \"was it heads and heads\", we would've needed only one question. But then we would've been lucky. *On average* we need two questions.\n</details>\n\n\n\n::: {.column-margin}\nBateson, G. (2000). \"Double Bind\" in *Steps to an Ecology of Mind: Collected Essays in Anthropology, Psychiatry, Evolution, and Epistemology*. University of Chicago Press.\n:::\nThis was the big innovation from Shannon. A **bit is the number of yes-no questions we need to ask in order to know an outcome in an experiment**. This is why Gregory Bateson called a bit \"the difference which makes a difference.\"  If we start with the bit $0$, the $0$ is the first \"difference\" which makes the difference that our sequence is one that starts with heads. If we add a $0$ to the bit sequence, the \"difference\" which is made determines that the full sequence is $HH$. Once we get to represent language with bits in the next section, this becomes less tautological.\n\nWhat is more, this average number of bits that we need to answer as yes-no question was, in fact, how Shannon **defined information**. Before we go on to formalize this discovery mathematically, let's build up some more intuition.\n\nLet's consider an experiment with three trials or tosses. Typing that out is annoying, so we'll use a helper function for it.\n\n**PROMPT:** How big is the sample space of eight trials? You can use ```produce_N_compitations``` to find out.\n\n**PROMPT:** How would we encode that in bits? Use ```produce_N_combinations``` again, if you want.\n\n<details>\n<summary><strong>PROMPT:</strong> How many questions do we need now?</summary><br>\n\nThree questions!\n\n**First question:** Is the first throw heads? If yes, we have $\\{HTT, HTH, HHT, HHH\\}$ left. If no, we have $\\{TTT, TTH, THT, THH \\}$ left. \n\nWith four options left, we know from above we need just two more questions. So three questions in total!\n</details>\n\n<details>\n<summary><b>PROMPT:</b> Do you notice some sort of connection between how we build our outcomes and the number of bits we need to describe the distribution?</summary>\n\nYes, there is indeed a connection. And that’s good, because all this counting can get tiresome…\n\nTo know how many options we have to choose among, we can use a simple formula. If $S$ is the number of symbols we can choose from (i.e. 2, because we choose between $H$ and $T$) and $n$ is the number of combinations of those (3 in the above example), then the number of basic outcomes $E$ (e.g. $HH$, $HT$) is:\n\n$E = S^n$\n</details>\n\nLet's try it out:\n\nTwo outcomes $H$ and $T$ can be combined combined in sequences of three in eight different ways, just as we saw above.\n\n**PROMPT:** How many bits will we need to represent these options?\n\nIf we already know $E$, we can find out how many bits we need by taking the *logarithm* of $E$, because the logarithm is simply the inverse of a power:\n\nIn fact, we can count the number of bits we need directly from $n$ and $S$:\n\nThis is also how Ralph Hartley, an engineer at Bell Labs, defined information in a paper in 1928. For him information was just:\n\n::: {.column-margin} \nHartley, R. V. L. (1928). Transmission of Information. Bell System Technical Journal, 7(3), 535–563. [[URL]](https://onlinelibrary.wiley.com/doi/abs/10.1002/j.1538-7305.1928.tb01236.x)\n<br>\n<br>\nNyquist, H. (1924). Certain factors affecting telegraph speed. The Bell System Technical Journal, 3(2), 324–346. The Bell System Technical Journal. [[URL]](https://doi.org/10.1002/j.1538-7305.1924.tb01361.x)\n\n\n:::\n\n$$\nn \\cdot \\log S\n$$\n\nBut this isn't the definition of that history settled on, and *only* applies to uniform distributions where all outcomes are equally likely. In fact, already before Shannon, Hartley and Ralph Nyquist at Bell Labs had defined \"intelligence\" in the above manner. While entirely satisfactory for uniform distributions, this definition does not work when we stop assuming all outcomes are equally probable. Furthermore, we have not even worked with actual probabilities yet! Instead, we have just assumed uniformity. In order to move away from that assumption, we have to define bits for probabilities.\n\nBefore we move on to deal with that in final part of this section, let's end this one by taking a little detour to understand the logarithm. If you are already familiar with logs, feels free to jump to the last part on bits for probabilities.\n\n#### Aside: What Does the Logarithm Do?\n\nNotice how we used `numpy.log2` in the previous section to calculate the number of bits needed to encode our sequence of coin tosses. In, fact, we use $log_2$ because it assumes $S=2$, 0 or 1, \"yes\" or \"no\". If we use some other logarithm, it's no longer the number of bits we are calculating. For instance, using a logarithm of the natural number $e$ would give us the necessary number of \"nats\".\n\n::: {.column-margin} \nFor a very approachable introduction to bits, including intuitive explanations of the logarithm, and information theory more broadly, I recommend the work of information theorist John Pierce.\n<br>\n<br>\nPierce, J. R. (1980). *An Introduction to Information Theory: Symbols, Signals and Noise*. (Second revised edition). Dover Books.\n::: \n\nThe logarithm to the base 2 of a number is the power to which 2 must be raised to equal the number. We can express the same thing mathematically:\n\n$$\n2^{\\log_2 x} = x\\\\\n$$\n\nWith our three tosses of heads and tails:\n\n$$\n\\log_2 8 = 3 \\\\\n2^{\\log_2 8} = 8 \\\\\n2^3 = 8 \n$$\n\nTwo must be raised to $3$ to give us 8.\n\n**PROMPT:** What are the logarithms of the following numbers: 1, 2, 4, 8, 16, 32, 64?\n\n| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n|---------------|---------------------|------------|\n| $1$           | $0$                 |$2^0 = 1$   |\n| $2$           | $1$                 |$2^1 = 2$   |\n| $4$           | $2$                 |$2^2 = 4$   |\n| $8$           | $3$                 |$2^3 = 8$   |\n| $16$          | $4$                 |$2^4 = 16$  |\n| $32$          | $5$                 |$2^5 = 32$  |\n| $64$          | $6$                 |$2^6 = 64$  |\n\nAs we can see when plotting the logarithms, the logarithm of a number $x$ first grows very quickly, but then slows down.\n\n### Bits for Probabilities\n\nSo far we have worked with *counts*, but we started out asking how we could encode *probabilities*. How could we make that leap? An interesting property with the logarithm, is that taking the log of a fraction gives us the same result as taking the log of the denominator *but negative*. So\n\n$$\n\\log_2 2 = 1\n$$\n\nAnd:\n\n$$\n\\log_2\\frac{1}{2} = - 1\n$$\n\nThis means that counting logs for a uniform distribution where all events are equally probable is just like working with counts, but negative!\n\n**PROMPT:** What are the logarithms of the following numbers: $\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}$\n\n| $\\mathbf{x}$  | $\\mathbf{\\log_2 x}$ | why?       |\n|---------------|---------------------|------------|\n| $1$           | $0$                 |$2^0 = 1$   |\n| $\\frac{1}{2}$           | $-1$                 |$2^{-1} = \\frac{1}{2}$   |\n| $\\frac{1}{4}$           | $-2$                 |$2^{-2} = \\frac{1}{4}$   |\n| $\\frac{1}{8}$           | $-3$                 |$2^{-3} = \\frac{1}{8}$   |\n| $\\frac{1}{16}$          | $-4$                 |$2^{-4} = \\frac{1}{16}$  |\n| $\\frac{1}{32}$          | $-5$                 |$2^{-5} = \\frac{1}{32}$  |\n| $\\frac{1}{64}$          | $-6$                 |$2^{-6} = \\frac{1}{64}$  |\n\nPlotting the log for our fractional $x$ values, we can note that the log grows very quickly as we start from very small values and move to larger ones, only to flatten out asymptotically when we approach $x$.\n\nOkay, given that we can use logarithms to count bits for probabilities, we are ready to move on to a more general definition of information that does not assume uniformity.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"include-in-header":["../hide-html-code.html"],"toc":true,"toc-depth":4,"output-file":"01_bits.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","monofont":"JetBrains Mono","theme":"cosmo","toc-location":"left","math":"mathjax","code-summary":"Code Toggle","title":"Bits: The Difference that Makes a Difference"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}