{"title":"Entropy: Expected Differences","markdown":{"yaml":{"title":"Entropy: Expected Differences"},"headingText":"Bits for (a very simple) Language","containsRefs":false,"markdown":"\n\n\n\n\n\n\n**The Big Question:** How do we most effectively code messages as signals?\n\nWe are now ready to understand the mathematical concept of **information entropy**. \n\nTo do so, let's return to the unbiased coin, once again. We already know we need two bits to encode it. BUt what if we wanted to calculate this value just from knowing the probabilities:\n\n$$\nP(H) = \\frac{1}{2} \\;\\;\\;\nP(T) = \\frac{1}{2}\n$$\n\nIf we take the log of these, we get:\n    \n$$\n\\log_2\\frac{1}{2} + \\log_2\\frac{1}{2} = \\\\\n-1 + (-1) = \\\\ \n-2\n$$\n\nBut that's too much! And negative! We know the bits to encode a fair coin is one. What we need to do is to weigh each event in our distribution by it's probability to get not just a sum but a **\"weighted sum\"**. \n$$\n\\begin{equation*}\n\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) = \\\\\n\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right) = \\\\\n-\\frac{1}{2} -\\frac{1}{2} = \\\\\n-1\n\\end{equation*}\n$$\n\n\n\nBetter! But still negative. What we do is just reverse the sign of the equation, like this:\n    \n$$\n\\begin{equation*}\n-1 \\times \\left[\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right)\\right] = \\\\\n-1 \\times \\left[\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right)\\right] = \\\\ \n-1 \\times \\left[-\\frac{1}{2} -\\frac{1}{2}\\right] = \\\\\n-1 \\times (-1) = \\\\\n1\n\\end{equation*}\n$$\n\n\nWe can actually use this to take a stab at a first definition of **information entropy**. We will use the symbol $\\mathbb{H}$. So let's imagine a distribution $X$ with two outcomes $x_1$ and $x_2$ and let's for simplicity's sake say they have probabilities $p_1$ and $p_2$. Then, the information entropy is:\n\n$$\n\\mathbb{H}(X) = -(p_1 \\log_2 p_1 + p_2 \\log_2 p_2))\n$$\n\nAnd we can actually generalize this to a case with any number of outcomes and with any probability values, like this:\n    \n$$\n\\begin{equation}\n  \\mathbb{H}(X) = -\\overbrace{\\sum_{i=1}^N P(x_i)}^\\text{Weighted sum} \\underbrace{\\log_2 P(x_i)}_\\text{Bits}\n\\end{equation}\n$$\n\nLet's unpack again.\n\n1) First, the intimidating symbol $\\sum_{i=1}^N$ just states that we sum over all the probabilities in our distribution, with $N$ outcomes $x_i$, each of them indexed with $i$. It's like going over all the values in a list of length $N$ in a for-loop and adding them, just like the symbol $\\prod$ did for multiplication. Because we are summing probabilities, what we get is a weighted sum, just like we wanted.\n\n2) Second, we calculate the bits required to encode each outcome. This we already went over above.\n\n3) Third, we take the negative value of the whole thing, because the logarithms of fractions are negative (and probabilities are always fractions, except at 0 and 1) and we want the actual bits, not a negative value.\n\n4) The $(X)$ after $\\mathbb{H}$ just says that we are going over an entire distribution. It's notation for \"random variable\", which for our purposes is synonymous with distribution. It's not necessary, but you will see that notation quite often when doing machine learning, so it's good to know it.\n\n**PROMPT:** How would you code entropy if you were to use a for loop? **Hint:** You will need to use ```np.log2```\n\nGreat! Let's wrap that in a function. $\\log_2 0$ is undefined, so we need to handle that separately.\n\nLet's now go over all these distributions and get the entropy for each of them. We'll need a list of distributions.\n\n**PROMPT** Get the entropy for each distribution in ```prob_distributions```. Save the output as a ```list```. <div>\n**Remember:** Each tuple in this list is a probability distribution.\n\nWe can also write our function in numpy, without the for-loop. It's a good exercise to do this, because for these types of functions people will rarely use for-loops.\n\n**PROMPT:** Re-write the entropy function in ```numpy```. <br>\n**Hints:** \n- It's one line of code for the entropy itself, but...\n- ... if you want to handle the zeroes (you don't have to), you can do this:\n\nLet's plot it to make sure it's the same as the for-loop version.\n\n And indeed it is! We've also added a red dot to highlight where the entropy as it's highest. \n - It's the point where $P(H)=0.5$, i.e. where $P(T)$ is also equal to $0.5$. \n - This is where the distribution is uniform. \n - This is why Weaver and Shannon called this least informative distribution the **\"maximum entropy\" distribution**.\n\nWhy is entropy maximized where the distribution is uniform? \n- Because this is where there is no structure, no redundancy.\n- All events are equally likely. \n- This hightlights Shannon's idea that information measures **suprise**. \n- When there is no structure, all events are equally surprising, so \"average surprise\" is maximized!\n\nEntropy is *not* at it's highest where probability is at it's highest, because:\n\n1) Probability measures the properties of *individual* events\n2) Information entropy measures the properties of *entire distributions*\n\n\nLet's now imagine an artificial language with only four letters, all equally probable. The letters are our unknown $x_i$ variable, and we index them with $i$ like this:\n\n$$\nx_1 = A, \\; x_2 = B, \\; x_3 = C, \\; x_4 = D\n$$\n\nThey also have corresponding probabilities, so that \n\n$$\np(x_1) = 0.25, \\; p(x_2) = 0.25, \\; p(x_3) = 0.25, \\; p(x_4) = 0.25\n$$\n\nLet's plot the language as a distribution\n\nOur language has a uniform distribution, so the line is flat. We've seen this type of event before: \n- When we were tossing our fair coin two times, we had four possible outcomes, just like now. \n- The average bits we needed to encode those outcomes were $2$. \n\nLet's see if that's true now as well:\n\nNo surprises here. Or, from Shannon's perspective, **only surprises**! \n- Each outcome requires two bits to encode. \n- To clarify, we can re-write our code for the bit table to include the probability of the events.\n\nBut what if our language instead had the following probabilities:\n    \n$$\n\\begin{aligned}\np(x_1) &= 0.5 \\\\\np(x_2) &= 0.25 \\\\\np(x_3) &= 0.125 \\\\\np(x_4) &= 0.125\n\\end{aligned}\n$$\n\nLet's plot that again.\n\nLooks familiar? Well, we've just created a language that seems to follow Zipf's Law. It's more simple than an actual language to make analysis easier, but the idea is the same:\n\nIf we now want to encode this language in bits, how do we go about it? Well, for starters we know how many bits we need on average. That knowledge is provided by our entropy function.\n\n\nBut we know more than that — each outcome needs a number of bits that corresponds to its probability:\n\n$$\n\\begin{aligned}\n\\text{bits for } A &= -\\log p(A) = -\\log\\frac{1}{2} = 1 \\\\\n\\text{bits for } B &= -\\log p(B) = -\\log\\frac{1}{4} = 2 \\\\\n\\text{bits for } C &= -\\log p(C) = -\\log\\frac{1}{8} = 3 \\\\\n\\text{bits for } D &= -\\log p(D) = -\\log\\frac{1}{8} = 3\n\\end{aligned}\n$$\n\nDoes this work? Let's try!\n\n**PROMPT:** Calculate how many bits we have on average, if we weigh each by their corresponding probability.\n\nIt matches the entropy! Let's make that into a function.\n\nIf we output our bit table with these values, we see what such an encoding could look like.\n\n**PROMPT** Create the list lang_bits with four entries. Make these correspond to the bits each letter should have:\n\n$$\n\\begin{aligned}\nA & : 1 \\\\\nB & : 2 \\\\\nC & : 3 \\\\\nD & : 3\n\\end{aligned}\n$$\n","srcMarkdownNoYaml":"\n\n\n\n\n\n\n**The Big Question:** How do we most effectively code messages as signals?\n\nWe are now ready to understand the mathematical concept of **information entropy**. \n\nTo do so, let's return to the unbiased coin, once again. We already know we need two bits to encode it. BUt what if we wanted to calculate this value just from knowing the probabilities:\n\n$$\nP(H) = \\frac{1}{2} \\;\\;\\;\nP(T) = \\frac{1}{2}\n$$\n\nIf we take the log of these, we get:\n    \n$$\n\\log_2\\frac{1}{2} + \\log_2\\frac{1}{2} = \\\\\n-1 + (-1) = \\\\ \n-2\n$$\n\nBut that's too much! And negative! We know the bits to encode a fair coin is one. What we need to do is to weigh each event in our distribution by it's probability to get not just a sum but a **\"weighted sum\"**. \n$$\n\\begin{equation*}\n\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) = \\\\\n\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right) = \\\\\n-\\frac{1}{2} -\\frac{1}{2} = \\\\\n-1\n\\end{equation*}\n$$\n\n\n\nBetter! But still negative. What we do is just reverse the sign of the equation, like this:\n    \n$$\n\\begin{equation*}\n-1 \\times \\left[\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right)\\right] = \\\\\n-1 \\times \\left[\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right)\\right] = \\\\ \n-1 \\times \\left[-\\frac{1}{2} -\\frac{1}{2}\\right] = \\\\\n-1 \\times (-1) = \\\\\n1\n\\end{equation*}\n$$\n\n\nWe can actually use this to take a stab at a first definition of **information entropy**. We will use the symbol $\\mathbb{H}$. So let's imagine a distribution $X$ with two outcomes $x_1$ and $x_2$ and let's for simplicity's sake say they have probabilities $p_1$ and $p_2$. Then, the information entropy is:\n\n$$\n\\mathbb{H}(X) = -(p_1 \\log_2 p_1 + p_2 \\log_2 p_2))\n$$\n\nAnd we can actually generalize this to a case with any number of outcomes and with any probability values, like this:\n    \n$$\n\\begin{equation}\n  \\mathbb{H}(X) = -\\overbrace{\\sum_{i=1}^N P(x_i)}^\\text{Weighted sum} \\underbrace{\\log_2 P(x_i)}_\\text{Bits}\n\\end{equation}\n$$\n\nLet's unpack again.\n\n1) First, the intimidating symbol $\\sum_{i=1}^N$ just states that we sum over all the probabilities in our distribution, with $N$ outcomes $x_i$, each of them indexed with $i$. It's like going over all the values in a list of length $N$ in a for-loop and adding them, just like the symbol $\\prod$ did for multiplication. Because we are summing probabilities, what we get is a weighted sum, just like we wanted.\n\n2) Second, we calculate the bits required to encode each outcome. This we already went over above.\n\n3) Third, we take the negative value of the whole thing, because the logarithms of fractions are negative (and probabilities are always fractions, except at 0 and 1) and we want the actual bits, not a negative value.\n\n4) The $(X)$ after $\\mathbb{H}$ just says that we are going over an entire distribution. It's notation for \"random variable\", which for our purposes is synonymous with distribution. It's not necessary, but you will see that notation quite often when doing machine learning, so it's good to know it.\n\n**PROMPT:** How would you code entropy if you were to use a for loop? **Hint:** You will need to use ```np.log2```\n\nGreat! Let's wrap that in a function. $\\log_2 0$ is undefined, so we need to handle that separately.\n\nLet's now go over all these distributions and get the entropy for each of them. We'll need a list of distributions.\n\n**PROMPT** Get the entropy for each distribution in ```prob_distributions```. Save the output as a ```list```. <div>\n**Remember:** Each tuple in this list is a probability distribution.\n\nWe can also write our function in numpy, without the for-loop. It's a good exercise to do this, because for these types of functions people will rarely use for-loops.\n\n**PROMPT:** Re-write the entropy function in ```numpy```. <br>\n**Hints:** \n- It's one line of code for the entropy itself, but...\n- ... if you want to handle the zeroes (you don't have to), you can do this:\n\nLet's plot it to make sure it's the same as the for-loop version.\n\n And indeed it is! We've also added a red dot to highlight where the entropy as it's highest. \n - It's the point where $P(H)=0.5$, i.e. where $P(T)$ is also equal to $0.5$. \n - This is where the distribution is uniform. \n - This is why Weaver and Shannon called this least informative distribution the **\"maximum entropy\" distribution**.\n\nWhy is entropy maximized where the distribution is uniform? \n- Because this is where there is no structure, no redundancy.\n- All events are equally likely. \n- This hightlights Shannon's idea that information measures **suprise**. \n- When there is no structure, all events are equally surprising, so \"average surprise\" is maximized!\n\nEntropy is *not* at it's highest where probability is at it's highest, because:\n\n1) Probability measures the properties of *individual* events\n2) Information entropy measures the properties of *entire distributions*\n\n## Bits for (a very simple) Language\n\nLet's now imagine an artificial language with only four letters, all equally probable. The letters are our unknown $x_i$ variable, and we index them with $i$ like this:\n\n$$\nx_1 = A, \\; x_2 = B, \\; x_3 = C, \\; x_4 = D\n$$\n\nThey also have corresponding probabilities, so that \n\n$$\np(x_1) = 0.25, \\; p(x_2) = 0.25, \\; p(x_3) = 0.25, \\; p(x_4) = 0.25\n$$\n\nLet's plot the language as a distribution\n\nOur language has a uniform distribution, so the line is flat. We've seen this type of event before: \n- When we were tossing our fair coin two times, we had four possible outcomes, just like now. \n- The average bits we needed to encode those outcomes were $2$. \n\nLet's see if that's true now as well:\n\nNo surprises here. Or, from Shannon's perspective, **only surprises**! \n- Each outcome requires two bits to encode. \n- To clarify, we can re-write our code for the bit table to include the probability of the events.\n\nBut what if our language instead had the following probabilities:\n    \n$$\n\\begin{aligned}\np(x_1) &= 0.5 \\\\\np(x_2) &= 0.25 \\\\\np(x_3) &= 0.125 \\\\\np(x_4) &= 0.125\n\\end{aligned}\n$$\n\nLet's plot that again.\n\nLooks familiar? Well, we've just created a language that seems to follow Zipf's Law. It's more simple than an actual language to make analysis easier, but the idea is the same:\n\nIf we now want to encode this language in bits, how do we go about it? Well, for starters we know how many bits we need on average. That knowledge is provided by our entropy function.\n\n\nBut we know more than that — each outcome needs a number of bits that corresponds to its probability:\n\n$$\n\\begin{aligned}\n\\text{bits for } A &= -\\log p(A) = -\\log\\frac{1}{2} = 1 \\\\\n\\text{bits for } B &= -\\log p(B) = -\\log\\frac{1}{4} = 2 \\\\\n\\text{bits for } C &= -\\log p(C) = -\\log\\frac{1}{8} = 3 \\\\\n\\text{bits for } D &= -\\log p(D) = -\\log\\frac{1}{8} = 3\n\\end{aligned}\n$$\n\nDoes this work? Let's try!\n\n**PROMPT:** Calculate how many bits we have on average, if we weigh each by their corresponding probability.\n\nIt matches the entropy! Let's make that into a function.\n\nIf we output our bit table with these values, we see what such an encoding could look like.\n\n**PROMPT** Create the list lang_bits with four entries. Make these correspond to the bits each letter should have:\n\n$$\n\\begin{aligned}\nA & : 1 \\\\\nB & : 2 \\\\\nC & : 3 \\\\\nD & : 3\n\\end{aligned}\n$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"include-in-header":["../hide-html-code.html"],"toc":true,"toc-depth":4,"output-file":"02_entropy.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","monofont":"JetBrains Mono","theme":"cosmo","toc-location":"left","math":"mathjax","code-summary":"Code Toggle","title":"Entropy: Expected Differences"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}