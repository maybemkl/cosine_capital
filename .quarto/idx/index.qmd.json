{"title":"Introduction","markdown":{"yaml":{"title":"Introduction","subtitle":"Understanding how the Noisy Channel metaphor underlies both bits and embeddings"},"containsRefs":false,"markdown":"\n\nThis website provides supplementary context for the articles [\"Cosine Capital: Large language models and the embedding of all things\"](https://journals.sagepub.com/doi/10.1177/20539517251386055) (Big Data & Society, October 2025) and [\"Taking AI into the Tunnels\"](https://www.e-flux.com/journal/151/652643/taking-ai-into-the-tunnels) (e-flux, January 2025), along with some upcoming articles on the same topic. More than that, it provides a comprehensive introduction to classical information theory as it was conceived by Claude Shannon and its connections and impacts on neural network and large language model (LLM) research today.\n\nInformation theory relies on a very particular metaphor that understand communication as a sort of \"conduit\": You encode a message with a key, send the encoded message to a receiver, who decodes it with the same key, and reads the message. For instance, if you write \"SOS\" in Morse code, they \"key\" is the table that tells you how many dots (.) and dashes (-) to use for the letters S and O. In this case, the encoded version of your message is \"...---...\", i.e. three dots, three dashes, and, again, three dots. \n\n::: {.column-margin}\nReddy, M. (1993). The conduit metaphor. In *Metaphor and thought* (2nd ed., pp. 164–201). Cambridge University Press.\n:::\n\nNo where is this logic as neatly encapsulated as in the \"The General Communications System\" deviced by Claude Shannon, who is widely concidered the father of modern information theory. In this diagram, a sender picks a message (SOS), encodes it (... --- ...) using an [appropriate device](https://en.wikipedia.org/wiki/Telegraph_key), sends it over a channel such as telegraph wires to a [receiver](https://en.wikipedia.org/wiki/Telegraph_sounder), that then decodes the original \"SOS\" message from the dots and dashes. In the process, physical distortion on the line might scramble the encoded message, perhaps converting it to \"... --- ..-\". In this case, the receiver would decode it as \"SOU\" since \"..-\" stands for \"U\" in Morse. The wrong message would be received.\n\n::: {.column-margin}\n![](img/shannon_general_communication_system_diagram.png)\nA \"General Communications System\". Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379–423. The Bell System Technical Journal. [[URL]](https://ieeexplore.ieee.org/document/6773024) \n:::\n\nWhile this technical definition of communication in itself is quite straightforward, the general metaphor has taken an astonishing hold of how we understand communication more broadly, as linguists like Michael Reddy and George Lakoff have highlighted. Already in his introduction to Shannon's seminal book on information theory, Warren Weaver suggested that words might encode intentions that are then decoded from the words. If the words are decoded wrong, false intentions are deciphered from them. Indeed, as I and others have argued elsewhere, this metaphor seems to guide the field of natural language processing (NLP) today, not least in how large language models (LLMs) are understood to model and replicate human language. \n\n::: {.column-margin}\nLakoff, G., & Johnson, M. (2008). *Metaphors we live by*. University of Chicago Press.\n<br>\n<br>\nBender, E. M., & Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, 5185–5198. [[URL]](https://doi.org/10.18653/v1/2020.acl-main.463)\n<br>\n<br>\nBrunila, M. (2025). Cosine capital: Large language models and the embedding of all things. *Big Data & Society*, 12(4). [[URL]](https://doi.org/10.1177/20539517251386055)\n:::\n\nOn this website, I will, in the first instance, use Shannon's diagram of a General Communications System to provide not only a *technical* introduction to classical information theory but also a *conceptual* map between these technical concepts and the conduit metaphor of communication. Starting from the encoding of words into the zeroes and ones of **bits**, I move to show how expected information in bits is quantified through **information entropy**, and compared through **relative entropy**, **information** or **Kullback-Leibler divergence**, and **cross-entropy**. On the way, I connect these technical concepts to conceptual readings of information theory by anthropologists such as Gregory Bateson. In the second instance, I show how these tools and metaphors have gained new force through their application in the encoder-decoder architectures that are implicitly or explicitly implemented in both small (Word2Vec) and large (Transformer) language models. At this time, in November 2025, only the first part on information theory has been written.\n\nIn this sense, this website serves at least two functions:\n\n- A technical, historical, and philosophical introduction to information theory\n- An appendix to my work in Big Data & Society and elsewhere\n\nTo cite this page, please use:\n\n<pre><code>@book{brunila2025,\n  author = {Brunila, Mikael},\n  title = {A Mathematical Theory of Communication},\n  year = {2025},\n  publisher = {University of Illinois Press}\n}</code></pre>","srcMarkdownNoYaml":"\n\nThis website provides supplementary context for the articles [\"Cosine Capital: Large language models and the embedding of all things\"](https://journals.sagepub.com/doi/10.1177/20539517251386055) (Big Data & Society, October 2025) and [\"Taking AI into the Tunnels\"](https://www.e-flux.com/journal/151/652643/taking-ai-into-the-tunnels) (e-flux, January 2025), along with some upcoming articles on the same topic. More than that, it provides a comprehensive introduction to classical information theory as it was conceived by Claude Shannon and its connections and impacts on neural network and large language model (LLM) research today.\n\nInformation theory relies on a very particular metaphor that understand communication as a sort of \"conduit\": You encode a message with a key, send the encoded message to a receiver, who decodes it with the same key, and reads the message. For instance, if you write \"SOS\" in Morse code, they \"key\" is the table that tells you how many dots (.) and dashes (-) to use for the letters S and O. In this case, the encoded version of your message is \"...---...\", i.e. three dots, three dashes, and, again, three dots. \n\n::: {.column-margin}\nReddy, M. (1993). The conduit metaphor. In *Metaphor and thought* (2nd ed., pp. 164–201). Cambridge University Press.\n:::\n\nNo where is this logic as neatly encapsulated as in the \"The General Communications System\" deviced by Claude Shannon, who is widely concidered the father of modern information theory. In this diagram, a sender picks a message (SOS), encodes it (... --- ...) using an [appropriate device](https://en.wikipedia.org/wiki/Telegraph_key), sends it over a channel such as telegraph wires to a [receiver](https://en.wikipedia.org/wiki/Telegraph_sounder), that then decodes the original \"SOS\" message from the dots and dashes. In the process, physical distortion on the line might scramble the encoded message, perhaps converting it to \"... --- ..-\". In this case, the receiver would decode it as \"SOU\" since \"..-\" stands for \"U\" in Morse. The wrong message would be received.\n\n::: {.column-margin}\n![](img/shannon_general_communication_system_diagram.png)\nA \"General Communications System\". Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379–423. The Bell System Technical Journal. [[URL]](https://ieeexplore.ieee.org/document/6773024) \n:::\n\nWhile this technical definition of communication in itself is quite straightforward, the general metaphor has taken an astonishing hold of how we understand communication more broadly, as linguists like Michael Reddy and George Lakoff have highlighted. Already in his introduction to Shannon's seminal book on information theory, Warren Weaver suggested that words might encode intentions that are then decoded from the words. If the words are decoded wrong, false intentions are deciphered from them. Indeed, as I and others have argued elsewhere, this metaphor seems to guide the field of natural language processing (NLP) today, not least in how large language models (LLMs) are understood to model and replicate human language. \n\n::: {.column-margin}\nLakoff, G., & Johnson, M. (2008). *Metaphors we live by*. University of Chicago Press.\n<br>\n<br>\nBender, E. M., & Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, 5185–5198. [[URL]](https://doi.org/10.18653/v1/2020.acl-main.463)\n<br>\n<br>\nBrunila, M. (2025). Cosine capital: Large language models and the embedding of all things. *Big Data & Society*, 12(4). [[URL]](https://doi.org/10.1177/20539517251386055)\n:::\n\nOn this website, I will, in the first instance, use Shannon's diagram of a General Communications System to provide not only a *technical* introduction to classical information theory but also a *conceptual* map between these technical concepts and the conduit metaphor of communication. Starting from the encoding of words into the zeroes and ones of **bits**, I move to show how expected information in bits is quantified through **information entropy**, and compared through **relative entropy**, **information** or **Kullback-Leibler divergence**, and **cross-entropy**. On the way, I connect these technical concepts to conceptual readings of information theory by anthropologists such as Gregory Bateson. In the second instance, I show how these tools and metaphors have gained new force through their application in the encoder-decoder architectures that are implicitly or explicitly implemented in both small (Word2Vec) and large (Transformer) language models. At this time, in November 2025, only the first part on information theory has been written.\n\nIn this sense, this website serves at least two functions:\n\n- A technical, historical, and philosophical introduction to information theory\n- An appendix to my work in Big Data & Society and elsewhere\n\nTo cite this page, please use:\n\n<pre><code>@book{brunila2025,\n  author = {Brunila, Mikael},\n  title = {A Mathematical Theory of Communication},\n  year = {2025},\n  publisher = {University of Illinois Press}\n}</code></pre>"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":true,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"include-in-header":["hide-html-code.html"],"toc":true,"toc-depth":4,"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.6.40","monofont":"JetBrains Mono","theme":"cosmo","toc-location":"left","math":"mathjax","code-summary":"Code Toggle","title":"Introduction","subtitle":"Understanding how the Noisy Channel metaphor underlies both bits and embeddings"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}