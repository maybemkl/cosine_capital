[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "From Bits to Embeddings ‚Äì A Critical Introduction to Information Theory",
    "section": "",
    "text": "This website provides supplementary context for the articles ‚ÄúCosine Capital: Large language models and the embedding of all things‚Äù (Big Data & Society, October 2025) and ‚ÄúTaking AI into the Tunnels‚Äù (e-flux, January 2025), along with some upcoming articles on the same topic. More than that, it provides a comprehensive introduction to classical information theory as it was conceived by Claude Shannon and its connections and impacts on neural network and large language model (LLM) research today.\nInformation theory relies on a very particular metaphor that understand communication as a sort of ‚Äúconduit‚Äù: You encode a message with a key, send the encoded message to a receiver, who decodes it with the same key, and reads the message. For instance, if you write ‚ÄúSOS‚Äù in Morse code, they ‚Äúkey‚Äù is the table that tells you how many dots (.) and dashes (-) to use for the letters S and O. In this case, the encoded version of your message is ‚Äú‚Ä¶‚Äî‚Ä¶‚Äù, i.e.¬†three dots, three dashes, and, again, three dots.\n\n\nReddy, M. (1993). The conduit metaphor. In Metaphor and thought (2nd ed., pp.¬†164‚Äì201). Cambridge University Press.\nNo where is this logic as neatly encapsulated as in the ‚ÄúThe General Communications System‚Äù deviced by Claude Shannon, who is widely concidered the father of modern information theory. In this diagram, a sender picks a message (SOS), encodes it (‚Ä¶ ‚Äî ‚Ä¶) using an appropriate device, sends it over a channel such as telegraph wires to a receiver, that then decodes the original ‚ÄúSOS‚Äù message from the dots and dashes. In the process, physical distortion on the line might scramble the encoded message, perhaps converting it to ‚Äú‚Ä¶ ‚Äî ..-‚Äù. In this case, the receiver would decode it as ‚ÄúSOU‚Äù since ‚Äú..-‚Äù stands for ‚ÄúU‚Äù in Morse. The wrong message would be received.\n\n\n A ‚ÄúGeneral Communications System‚Äù. Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379‚Äì423. The Bell System Technical Journal. [URL]\nWhile this technical definition of communication in itself is quite straightforward, the general metaphor has taken an astonishing hold of how we understand communication more broadly, as linguists like Michael Reddy and George Lakoff have highlighted. Already in his introduction to Shannon‚Äôs seminal book on information theory, Warren Weaver suggested that words might encode intentions that are then decoded from the words. If the words are decoded wrong, false intentions are deciphered from them. Indeed, as I and others have argued elsewhere, this metaphor seems to guide the field of natural language processing (NLP) today, not least in how large language models (LLMs) are understood to model and replicate human language.\n\n\nLakoff, G., & Johnson, M. (2008). Metaphors we live by. University of Chicago Press.   Bender, E. M., & Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185‚Äì5198. [URL]   Brunila, M. (2025). Cosine capital: Large language models and the embedding of all things. Big Data & Society, 12(4). [URL]\nOn this website, I will, in the first instance, use Shannon‚Äôs diagram of a General Communications System to provide not only a technical introduction to classical information theory but also a conceptual map between these technical concepts and the conduit metaphor of communication. Starting from the encoding of words into the zeroes and ones of bits, I move to show how expected information in bits is quantified through information entropy, and compared through relative entropy, information or Kullback-Leibler divergence, and cross-entropy. On the way, I connect these technical concepts to conceptual readings of information theory by anthropologists such as Gregory Bateson. In the second instance, I show how these tools and metaphors have gained new force through their application in the encoder-decoder architectures that are implicitly or explicitly implemented in both small (Word2Vec) and large (Transformer) language models. At this time, in November 2025, only the first part on information theory has been written.\nIn this sense, this website serves at least two functions:\n\nA technical, historical, and philosophical introduction to information theory\nAn appendix to my work in Big Data & Society and elsewhere\n\nTo cite this page, please use:\n@book{brunila2025,\n  author = {Brunila, Mikael},\n  title = {A Mathematical Theory of Communication},\n  year = {2025},\n  publisher = {University of Illinois Press}\n}",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html",
    "href": "notebooks/01_bits.html",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "",
    "text": "2.1 The General Commmunications System\nRecall from the introdution the ‚Äúconduit metaphor‚Äù that guides not just information theory but more broadly many of our received notions of communications today. As I wrote, this metaphor is most neatly encapsulated in the ‚ÄúGeneral Communication System‚Äù deviced by Claude Shannon in a seminal 1948 paper. Shannon envisioned communication as a system with distinct analytical parts, which you can explore in this interactive figure:\nINFORMATION\n      SOURCE\n      MESSAGE\n    \n\n    \n      \n      TRANSMITTER\n      SIGNAL\n    \n\n    \n      \n      RECEIVED\n      SIGNAL\n    \n\n    \n      \n      RECEIVER\n      MESSAGE\n    \n\n    \n      \n      DESTINATION\n    \n\n    \n      \n      NOISE SOURCE\nWhat we have here, first, is an information source that selects a message out of a finite set of possible messages. For instance, this might be a person typing on a keyboard. In fact, the message could come from any stochastic and ergodic process, which simply means a random process which, over time, is regular enough to form statistical patterns.\nNext, these messages are encoded by a transmitter which converts them into a code. We saw the example of Morse in the introduction. However, here we will be looking at the encoding of messages as the bits, zeroes and ones. When typing on a keyboard, our computer does this for us.\nOnce the message is encoded as bits, it can be sent over as a signal over a channel. Effectively, the ‚Äúchannel‚Äù is all the material infrastructdure between a sender and a receiver, including in our example fiberoptic cables, routers, internet protocols, etc. In this process, some noise might be introduced to the signal. For instance, maybe some bits are scrambled when a data package is lost or perhaps a malicious third party modifies the signal.\nBe that as it may, we hopefully reach a receiver that decodes the message, usually by running the inverse operation of the transmitter. In our example, bits are turned into words and the words reach their destination, which Weaver and Shannon (1963, 33‚Äî34, 57) defined as ‚Äú[t]he person or thing for whom the message is intended.‚Äù\nIn this chapter, we will start from the encoding of words as bits in the transmitter. Why bits? And what do we have to assume of language to encode it like that?",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#the-general-commmunications-system",
    "href": "notebooks/01_bits.html#the-general-commmunications-system",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "",
    "text": "Shannon, C. E. (1948). A Mathematical Theory of Communication. The Bell System Technical Journal, 27(3), 379‚Äì423. The Bell System Technical Journal. [URL]\n\n\n\n\n\n\n\nShannon, C. E., & Weaver, W. (1963). The Mathematical Theory of Communication (1st paperback). University of Illinois Press.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#the-information-source-stochastic-parrots",
    "href": "notebooks/01_bits.html#the-information-source-stochastic-parrots",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "2.2 The Information Source: Stochastic Parrots",
    "text": "2.2 The Information Source: Stochastic Parrots\nInformation theory as we know it starts from cryptographic work done during World War II by Shannon and others. Whereas the early information theory of the interwar era by people such as Ralph Hartley and Harry Nyquist treated language as a sort of constant to be dealt with in the domain of telephony and telegraphy, Shannon took a different approach. Leaning on insights by earier cryptographers, Shannon approach language as ‚Äústochastic‚Äù and ‚Äúergodic‚Äù process. What this suggests is simply that there is statistical regularity to how frequent words are in text.\n\n\n\nA language is considered for cryptographic purposes to be a stochastic process which produces a discrete sequence of symbols in accordance with some systems of probabilities.\n\nShannon, C. E. (1945). A Mathematical Theory of Cryptography‚ÄîCase 10878. Bell Telephone Laboratories, Princeton Libraries. (p.¬†2)\nWhat this implies is a strong hypothesis about how language works and about the source that produces the inputs in the General System of Communication. Rather than people choosing words to speak at will, language itself appears as a constraint to which we have to adapt as we speak. Indeed, it is almost as if it was language that used us, rather than the other way around. Before we dive into the world of bits, we need to dwell on this conceptualization of the source that produces the inputs to its systems. This section is dedicated to that task.\n\n\nGeoghegan, B. D. (2019). Architectures of information‚ÄîA comparison of Wiener‚Äôs and Shannon‚Äôs theories of information. In T. Vardouli & O. Touloumi (Eds.), Computer Architectures: Constructing the Common Ground. Routledge. [URL]\n\n\n\n\n  \n    \n      \n        \n      \n    \n\n    \n    \n    \n    \n    \n    \n\n    \n    \n      \n      INFORMATION\n      SOURCE\n      MESSAGE\n    \n\n    \n      \n      TRANSMITTER\n      SIGNAL\n    \n\n    \n      \n      RECEIVED\n      SIGNAL\n    \n\n    \n      \n      RECEIVER\n      MESSAGE\n    \n\n    \n      \n      DESTINATION\n    \n\n    \n      \n      NOISE SOURCE\n    \n  \n\n  \n\n\n\n\n\n\n\n\nIn a famous critique of LLMs, Emily Bender and co-authors described these and other language models (LMs) as a ‚Äúsystem for haphazardly stitching together sequences of linguistic form‚Äù:\n\n\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ü¶ú Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610‚Äì623. [URL]\n\nContrary to how it may seem when we observe its output, an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning a stochastic parrot.\n\nWhat does it mean? Simply that LMs draw on observed probabilities of words and their co-occurence to form the sentences that we see on services like ChatGPT and Gemini. They treat language as a probability distribution. While LLMs find complex dependencies in how words co-occur over long passages of text, we will start with a much simpler representation of language.\n\n2.2.1 Zipf‚Äôs Law\nThis idea of treating language probabilistically is very old. Already in 1932, the linguist George Zipf had observed that word frequency is inversely proportional to work rank, a regularity which later became known as ‚ÄúZipf‚Äôs law‚Äù. Zipf‚Äôs Law is an example of one of the most simple language models, an early representation of language as a stochastic process. Zipf‚Äôs Law is interesting, because it is based on observed frequencies of words. In this sense, it is a herald of the empirical approach to language that predominates in language modelling today. Using these observed probabilities of individual words, we can already build a simple language model. Let us do just that using Lewis Carroll‚Äôs ‚ÄúAlice in Wonderland‚Äù. For this purpose, we will use the NLTK library to download the text and to ‚Äútokenize‚Äù it into individual words.\n\n\nCode Toggle\nimport math\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\n\nfrom collections import Counter\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import gutenberg\nfrom nltk.tokenize import RegexpTokenizer\n\nnltk.download('punkt_tab')\n\n\n\n\nCode Toggle\nimport sys\nsys.path.append('../py')\n\nfrom plots import (\n    plot_word_dist, \n    plot_log,\n    plot_probability_sequence,\n    compare_distributions_in_plot\n)\n\nsns.set_style(\"ticks\")\n\n\nUsing a custom function we can see how this tokenization works:\n\n\nCode Toggle\ndef tokens_from_gutenberg_text(txt_name):\n    sents = gutenberg.sents(txt_name)\n    words = [word for sent in sents for word in sent]\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(\" \".join(words).lower())\n    return tokens\n\nalice_tokens = tokens_from_gutenberg_text('carroll-alice.txt')\nalice_tokens[:10]\n\n\n['alice',\n 's',\n 'adventures',\n 'in',\n 'wonderland',\n 'by',\n 'lewis',\n 'carroll',\n '1865',\n 'chapter']\n\n\nWe can also count the words, to get an initial sense of what kind of distribution to expect:\n\n\nCode Toggle\nalice_counts = Counter(alice_tokens)\nalice_counts.most_common(10)\n\n\n[('the', 1642),\n ('and', 872),\n ('to', 729),\n ('a', 632),\n ('it', 595),\n ('she', 553),\n ('i', 543),\n ('of', 514),\n ('said', 462),\n ('you', 411)]\n\n\n\n\n2.2.2 A simple language model\nWhat do these counts mean in terms of probabilities? To estimate that, we count how many times each token appears (its frequency) and then divide that by the total number of tokens.\n\n\nCode Toggle\ndef get_probs_from_counter(counts: Counter[str]) -&gt; Counter[str]:\n    total = sum(counts.values())\n    probs = Counter({token: count / total for token, count in counts.items()})\n    return probs\n\n\nThe top 10 most common words in Alice in Wonderland are mostly so called ‚Äústopwords‚Äù (the, and, to, a, etc.) and words relating to dialogue:\n\n\nCode Toggle\nalice_probs = get_probs_from_counter(alice_counts)\nalice_probs.most_common(10)\n\n\n[('the', 0.06006731050629207),\n ('and', 0.03189932689493708),\n ('to', 0.026668129938542583),\n ('a', 0.023119695639449808),\n ('it', 0.021766169154228857),\n ('she', 0.020229733684518584),\n ('i', 0.019863915715539946),\n ('of', 0.0188030436055019),\n ('said', 0.016900790166812993),\n ('you', 0.01503511852502195)]\n\n\nTogether, the probabilities of these words form a probability distribution. For the 15 most common words, the distribution looks like this:\n\n\nCode Toggle\nplot_word_dist(alice_probs, 'bar')\n\n\n\n\n\n\n\n\n\nOnce we have a distribution of observed probabilities, we can also draw from it. This means that we sample from our representation of language to produce a sequence of words. This is, effectively, what large language models also do, but with a lot more complicated representations. Nevertheless, what we have at hand is a de facto language model.\nTo draw a fifty word sentence from our representation of language as it appears in Alice in Wonderland, we can run the following code:\n\n\nCode Toggle\nimport random\nrandom.seed(42)\n\noutcomes = list(alice_probs.keys())\nsample = random.choices(outcomes, weights=alice_probs.values(), k=50)\nprint(\" \".join(sample))\n\n\nrather in into and cut eat wandered the think in and some in her zealand from and said live alice his been a to king a the the him must his hurry they worried would put offended you sighing which legged i and it the and the but people could\n\n\nNot a great representation! But it‚Äôs also not entirely random. The word ‚Äúthe‚Äù appears over and over again, simply because it appears the most often in this (and any given English) text. However, we could work with a completely random sample as well. In this case, we would just give each word the same probability.\n\n\nCode Toggle\ndef probs_to_uniform(top_probs, n, n_total):\n    keys = [k for k, _ in top_probs.most_common(n)]\n    uniform_prob = 1 / n_total\n    return Counter({k: uniform_prob for k in keys})\n\nn_tokens = len(alice_tokens)\nn_types = len(list(alice_counts.keys()))\nalice_uniform = probs_to_uniform(alice_probs, n_types, n_types)\nalice_uniform.most_common(10)\n\n\n[('the', 0.00038895371450797355),\n ('and', 0.00038895371450797355),\n ('to', 0.00038895371450797355),\n ('a', 0.00038895371450797355),\n ('it', 0.00038895371450797355),\n ('she', 0.00038895371450797355),\n ('i', 0.00038895371450797355),\n ('of', 0.00038895371450797355),\n ('said', 0.00038895371450797355),\n ('you', 0.00038895371450797355)]\n\n\n\n\nCode Toggle\noutcomes = list(alice_uniform.keys())\nsample = random.choices(outcomes, weights=alice_uniform.values(), k=50)\nprint(\" \".join(sample))\n\n\nfloor fell unimportant justice favoured fender live tougher watch remarks believed farmer wig hoped furrows roughly wow way sigh worth jumped fifteenth warning shoulder normans lived anywhere easy knee sudden undertone waving hurrying classics guests race loving twist turned other suddenly houses growls wind seemed girls pictured closed queerest appealed\n\n\nNo the word ‚Äúthe‚Äù doesn‚Äôt appear a single time! Visually, the probability distribution now looks like this for the first 18 words:\n\n\nCode Toggle\nplot_word_dist(alice_uniform, 'bar')\n\n\n\n\n\n\n\n\n\nUnlike our representation using observed word probabilities, we use a simplifying assumption here. We loose all sense of structure, everything becomes uniform. By contrast, the observed probabilities already had some redundancy. We know, for instance, that by always picking the word ‚Äúthe‚Äù we will be more likely to correctly predict a draw from the text than always picking the word ‚Äúalice‚Äù.\n\n\n\nThe redundancy, on the other hand, measures the amount of constraint imposed on a text in the language due to its statistical structure, e.g., in English the high frequency of the letter E, the strong tendency of H to follow T or of L‚Äô to follow Q.\n\nShannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50‚Äì64. [URL]",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#a-simple-language-model",
    "href": "notebooks/01_bits.html#a-simple-language-model",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "2.3 A simple language model",
    "text": "2.3 A simple language model\nWhat do these counts mean in terms of probabilities? To estimate that, we count how many times each token appears (its frequency) and then divide that by the total number of tokens.\n\n\nCode Toggle\ndef get_probs_from_counter(counts: Counter[str]) -&gt; Counter[str]:\n    total = sum(counts.values())\n    probs = Counter({token: count / total for token, count in counts.items()})\n    return probs\n\n\nThe top 10 most common words in Alice in Wonderland are mostly so called ‚Äústopwords‚Äù (the, and, to, a, etc.) and words relating to dialogue:\n\n\nCode Toggle\nalice_probs = get_probs_from_counter(alice_counts)\nalice_probs.most_common(10)\n\n\n[('the', 0.06006731050629207),\n ('and', 0.03189932689493708),\n ('to', 0.026668129938542583),\n ('a', 0.023119695639449808),\n ('it', 0.021766169154228857),\n ('she', 0.020229733684518584),\n ('i', 0.019863915715539946),\n ('of', 0.0188030436055019),\n ('said', 0.016900790166812993),\n ('you', 0.01503511852502195)]\n\n\nTogether, the probabilities of these words form a probability distribution. For the 15 most common words, the distribution looks like this:\n\n\nCode Toggle\nplot_word_dist(alice_probs, 'bar')\n\n\n\n\n\n\n\n\n\nOnce we have a distribution of observed probabilities, we can also draw from it. This means that we sample from our representation of language to produce a sequence of words. This is, effectively, what large language models also do, but with a lot more complicated representations. Nevertheless, what we have at hand is a de facto language model.\nTo draw a fifty word sentence from our representation of language as it appears in Alice in Wonderland, we can run the following code:\n\n\nCode Toggle\nimport random\nrandom.seed(42)\n\noutcomes = list(alice_probs.keys())\nsample = random.choices(outcomes, weights=alice_probs.values(), k=50)\nprint(\" \".join(sample))\n\n\nrather in into and cut eat wandered the think in and some in her zealand from and said live alice his been a to king a the the him must his hurry they worried would put offended you sighing which legged i and it the and the but people could\n\n\nNot a great representation! But it‚Äôs also not entirely random. The word ‚Äúthe‚Äù appears over and over again, simply because it appears the most often in this (and any given English) text. However, we could work with a completely random sample as well. In this case, we would just give each word the same probability.\n\n\nCode Toggle\ndef probs_to_uniform(top_probs, n, n_total):\n    keys = [k for k, _ in top_probs.most_common(n)]\n    uniform_prob = 1 / n_total\n    return Counter({k: uniform_prob for k in keys})\n\nn_tokens = len(alice_tokens)\nn_types = len(list(alice_counts.keys()))\nalice_uniform = probs_to_uniform(alice_probs, n_types, n_types)\nalice_uniform.most_common(10)\n\n\n[('the', 0.00038895371450797355),\n ('and', 0.00038895371450797355),\n ('to', 0.00038895371450797355),\n ('a', 0.00038895371450797355),\n ('it', 0.00038895371450797355),\n ('she', 0.00038895371450797355),\n ('i', 0.00038895371450797355),\n ('of', 0.00038895371450797355),\n ('said', 0.00038895371450797355),\n ('you', 0.00038895371450797355)]\n\n\n\n\nCode Toggle\noutcomes = list(alice_uniform.keys())\nsample = random.choices(outcomes, weights=alice_uniform.values(), k=50)\nprint(\" \".join(sample))\n\n\nfloor fell unimportant justice favoured fender live tougher watch remarks believed farmer wig hoped furrows roughly wow way sigh worth jumped fifteenth warning shoulder normans lived anywhere easy knee sudden undertone waving hurrying classics guests race loving twist turned other suddenly houses growls wind seemed girls pictured closed queerest appealed\n\n\nNo the word ‚Äúthe‚Äù doesn‚Äôt appear a single time! Visually, the probability distribution now looks like this:\n\n\nCode Toggle\nplot_word_dist(alice_uniform, 'bar')\n\n\n\n\n\n\n\n\n\nUnlike our representation using observed word probabilities, we use a simplifying assumption here. We loose all sense of structure, everything becomes uniform. By contrast, the observed probabilities already had some redundancy. We know, for instance, that by always picking the word ‚Äúthe‚Äù we will be more likely to correctly predict a draw from the text than always picking the word ‚Äúalice‚Äù.\n\n\n\nThe redundancy, on the other hand, measures the amount of constraint imposed on a text in the language due to its statistical structure, e.g., in English the high frequency of the letter E, the strong tendency of H to follow T or of L‚Äô to follow Q.\n\nShannon, C. E. (1951). Prediction and Entropy of Printed English. Bell System Technical Journal, 30(1), 50‚Äì64. [URL]",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#the-transmitter-words-as-bits",
    "href": "notebooks/01_bits.html#the-transmitter-words-as-bits",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "2.3 The Transmitter: Words as Bits",
    "text": "2.3 The Transmitter: Words as Bits\nOkay, now we‚Äôve established some key things: - We know what a distribution is - We know that Shannon and other pioneers of cybernetics thought of language as a stochastic process applying to discrete symbols and words - Ergo: Language was a distribution for them - If we know some of the statistical regularities or the ‚Äúredundancies‚Äù of this distribution we can work on problems like cryptography and machine translation. - Zipf‚Äôs Law is an example of a very simple approximation of such redundancies, based solely on a generalization from observed word frequencies.\nHowever, in order to do anything practical with this hypothesis about the source, our distributions of interested have to be somehow represented and encoded in order to be transmitted. Let‚Äôs revisit the image of the noisy channel. In Morse code, we would encode our words using the long and short codes, encoded as such by a telegraph key. This would be our transmitter. However, Morse is not the most efficient way to represent language.\n\n\n\n\n  \n    \n      \n        \n      \n    \n\n    \n    \n    \n    \n    \n    \n\n    \n    \n      \n      INFORMATION\n      SOURCE\n      MESSAGE\n    \n\n    \n      \n      TRANSMITTER\n      SIGNAL\n    \n\n    \n      \n      RECEIVED\n      SIGNAL\n    \n\n    \n      \n      RECEIVER\n      MESSAGE\n    \n\n    \n      \n      DESTINATION\n    \n\n    \n      \n      NOISE SOURCE\n    \n  \n\n  \n\n\n\n\n\n\n\n\nBasing his ideas on innovations in ‚ÄúBoolean algebra‚Äù, Shannon decided that the most intuitive way to represent anything was the most simple one: as ‚Äúyes‚Äù or ‚Äúno‚Äù, 0 or 1. A poll in the coffee room of Bell Labs‚Äìwhere Shannon worked at the time-established that this unit should be known as a ‚Äúbit‚Äù of information.\n\n\nShannon, C. E. (1940). A symbolic analysis of relay and switching circuits [Thesis, Massachusetts Institute of Technology]. [URL]\n\n2.3.1 The most simple language\nIn his original papers, Shannon demonstrated the first principles of information theory using a coin toss, only then moving on to human language. We will follow Shannon there. In fact, a coin toss might be one of the most simple stochastic processes, as it only has two outcomes (heads or tails) and, if the coin is fair, no structure or redundancy.\nIn order to work with the probabilities of a coin toss in Python, we are going to use numpy to generate probability distributions and show the behavior of coins that are fair (heads and tails are equally likely) and biased (heads is more likely than tails, or vice versa.).\nUsing the code below, we can generate the probabilities of a coin that is biased towards tails (the first five values) or heads (the last five values).\n\n\nCode Toggle\np_heads = np.linspace(0, 1, 9)\np_heads\n\n\narray([0.   , 0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875, 1.   ])\n\n\n\n\nCode Toggle\nplot_probability_sequence(p_heads)\n\n\n\n\n\n\n\n\n\nAbove, we let the probability of heads increase in small increments from 0 to 1. That gives us a list of different values for \\(P(H)\\), for different ‚Äúcoins‚Äù with different biases, if you will. Now we can get a similar sized but inverse list of \\(P(T)\\) by just taking \\(1 - P(H)\\).\nPROMPT: Generate p_heads using linspace and then use p_heads to create p_tails.\n\n\nCode Toggle\np_tails = 1 - p_heads\n\n\nPlotting the probabilities of heads and tails for each of our nine coins, we can see how the probability of one falls as the probability for the other goes up.\n\n\nCode Toggle\ncompare_distributions_in_plot(list(range(1, 10)),\n                              p_heads, \n                              p_tails,\n                              r'$P(H)$',\n                              r'$P(T)$',\n                              \"Coin\",\n                              \"Probability\",\n                              \"Probabilities of heads and tails\")\n\n\n\n\n\n\n\n\n\nThis might seem stupidly self-evident, but there is a point here: We are establishing a very simple baseline from which to build up our understanding of bits and information. In fact, how could we represent the possible outcomes of a single and fair coin toss as bits?\nPROMPT: If we want to represent a single, fair coin toss as bits, how do we proceed? How many bits do we need in total?\n\n\nCode Toggle\ndef print_simple_bit_table(outcome, bits):\n    n_bits = [len(b) for b in bits]\n    \n    return pd.DataFrame(\n        {'outcomes':outcome, \n         'bits':bits, \n         'n_bits':n_bits}).style.hide()\n\n\n\n\nCode Toggle\nprint_simple_bit_table(['heads', 'tails'], ['0', '1'])\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nheads\n0\n1\n\n\ntails\n1\n1\n\n\n\n\n\nAs you can see, we can represent heads as 0 and tails as 1. And, in total, we need two bits to represent the sample space of a single toss of a fair coin. Using notation from probability and set theory, we might write:\n\\(A = \\{H, T\\}\\)\nOur sample space consists of just two options, and we only need two bits to represent it (assuming each outcome is equally likely!).\n\n\nPROMPT: Let‚Äôs move up. What is the sample space of two successive coin tosses of a fair coin?\n\n\n\\(A = \\{HH, HT, TT, TH \\}\\)\n\nPROMPT: If we want to represent this sample space of two coin tosses in bits, how do we do it?\n\n\nCode Toggle\ntwo_toss_combinations = ['HH', 'HT', 'TT', 'TH']\ntwo_bit_combinations = ['00', '01', '10', '11']\n\nprint_simple_bit_table(two_toss_combinations, two_bit_combinations)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nHH\n00\n2\n\n\nHT\n01\n2\n\n\nTT\n10\n2\n\n\nTH\n11\n2\n\n\n\n\n\n\n\nPROMPT: So far, we have been counting total bits. However, how many questions would we need on average to find out what the outcome was in the last experiment?\n\n Two questions!\nFirst question: Is the first throw heads? If yes, we have \\(\\{HH, HT\\}\\) left. If no, we have \\(\\{TT, TH \\}\\) left.\nSecond question: Is the first throw heads? Whatever the answer, only one option will remain.\nOf course we could get lucky by asking just one question. Say, for example, the outcome was \\(HH\\). If we ask ‚Äúwas it heads and heads‚Äù, we would‚Äôve needed only one question. But then we would‚Äôve been lucky. On average we need two questions.\n\n\n\nBateson, G. (2000). ‚ÄúDouble Bind‚Äù in Steps to an Ecology of Mind: Collected Essays in Anthropology, Psychiatry, Evolution, and Epistemology. University of Chicago Press.\nThis was the big innovation from Shannon. A bit is the number of yes-no questions we need to ask in order to know an outcome in an experiment. This is why Gregory Bateson called a bit ‚Äúthe difference which makes a difference.‚Äù If we start with the bit \\(0\\), the \\(0\\) is the first ‚Äúdifference‚Äù which makes the difference that our sequence is one that starts with heads. If we add a \\(0\\) to the bit sequence, the ‚Äúdifference‚Äù which is made determines that the full sequence is \\(HH\\). Once we get to represent language with bits in the next section, this becomes less tautological.\nWhat is more, this average number of bits that we need to answer as yes-no question was, in fact, how Shannon defined information. Before we go on to formalize this discovery mathematically, let‚Äôs build up some more intuition.\nLet‚Äôs consider an experiment with three trials or tosses. Typing that out is annoying, so we‚Äôll use a helper function for it.\n\n\nCode Toggle\nfrom itertools import product\n\ndef produce_N_combinations(items, N):\n    return [''.join(x) for x in product(items, repeat = N)]\n\n\nPROMPT: How big is the sample space of eight trials? You can use produce_N_compitations to find out.\n\n\nCode Toggle\nthree_toss_combinations = produce_N_combinations(['T', 'H'], 3)\nthree_toss_combinations\n\n\n['TTT', 'TTH', 'THT', 'THH', 'HTT', 'HTH', 'HHT', 'HHH']\n\n\nPROMPT: How would we encode that in bits? Use produce_N_combinations again, if you want.\n\n\nCode Toggle\nthree_bit_combinations = produce_N_combinations(['0', '1'], 3)\nthree_bit_combinations\n\n\n['000', '001', '010', '011', '100', '101', '110', '111']\n\n\n\n\nCode Toggle\nprint_simple_bit_table(three_toss_combinations, \n                       three_bit_combinations)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nTTT\n000\n3\n\n\nTTH\n001\n3\n\n\nTHT\n010\n3\n\n\nTHH\n011\n3\n\n\nHTT\n100\n3\n\n\nHTH\n101\n3\n\n\nHHT\n110\n3\n\n\nHHH\n111\n3\n\n\n\n\n\n\n\nPROMPT: How many questions do we need now?\n\n\nThree questions!\nFirst question: Is the first throw heads? If yes, we have \\(\\{HTT, HTH, HHT, HHH\\}\\) left. If no, we have \\(\\{TTT, TTH, THT, THH \\}\\) left.\nWith four options left, we know from above we need just two more questions. So three questions in total!\n\n\n\nPROMPT: Do you notice some sort of connection between how we build our outcomes and the number of bits we need to describe the distribution?\n\nYes, there is indeed a connection. And that‚Äôs good, because all this counting can get tiresome‚Ä¶\nTo know how many options we have to choose among, we can use a simple formula. If \\(S\\) is the number of symbols we can choose from (i.e.¬†2, because we choose between \\(H\\) and \\(T\\)) and \\(n\\) is the number of combinations of those (3 in the above example), then the number of basic outcomes \\(E\\) (e.g.¬†\\(HH\\), \\(HT\\)) is:\n\\(E = S^n\\)\n\nLet‚Äôs try it out:\n\n\nCode Toggle\nS = 2\nn = 3\n\n\n\n\nCode Toggle\nE = np.power(S, n)\nE.item()\n\n\n8\n\n\nTwo outcomes \\(H\\) and \\(T\\) can be combined combined in sequences of three in eight different ways, just as we saw above.\nPROMPT: How many bits will we need to represent these options?\nIf we already know \\(E\\), we can find out how many bits we need by taking the logarithm of \\(E\\), because the logarithm is simply the inverse of a power:\n\n\nCode Toggle\nnp.log2(E).item()\n\n\n3.0\n\n\nIn fact, we can count the number of bits we need directly from \\(n\\) and \\(S\\):\n\n\nCode Toggle\nn*np.log2(S).item()\n\n\n3.0\n\n\nThis is also how Ralph Hartley, an engineer at Bell Labs, defined information in a paper in 1928. For him information was just:\n\n\nHartley, R. V. L. (1928). Transmission of Information. Bell System Technical Journal, 7(3), 535‚Äì563. [URL]   Nyquist, H. (1924). Certain factors affecting telegraph speed. The Bell System Technical Journal, 3(2), 324‚Äì346. The Bell System Technical Journal. [URL]\n\\[\nn \\cdot \\log S\n\\]\nBut this isn‚Äôt the definition of that history settled on, and only applies to uniform distributions where all outcomes are equally likely. In fact, already before Shannon, Hartley and Ralph Nyquist at Bell Labs had defined ‚Äúintelligence‚Äù in the above manner. While entirely satisfactory for uniform distributions, this definition does not work when we stop assuming all outcomes are equally probable. Furthermore, we have not even worked with actual probabilities yet! Instead, we have just assumed uniformity. In order to move away from that assumption, we have to define bits for probabilities.\nBefore we move on to deal with that in final part of this section, let‚Äôs end this one by taking a little detour to understand the logarithm. If you are already familiar with logs, feels free to jump to the last part on bits for probabilities.\n\n2.3.1.1 Aside: What Does the Logarithm Do?\nNotice how we used numpy.log2 in the previous section to calculate the number of bits needed to encode our sequence of coin tosses. In, fact, we use \\(log_2\\) because it assumes \\(S=2\\), 0 or 1, ‚Äúyes‚Äù or ‚Äúno‚Äù. If we use some other logarithm, it‚Äôs no longer the number of bits we are calculating. For instance, using a logarithm of the natural number \\(e\\) would give us the necessary number of ‚Äúnats‚Äù.\n\n\nFor a very approachable introduction to bits, including intuitive explanations of the logarithm, and information theory more broadly, I recommend the work of information theorist John Pierce.   Pierce, J. R. (1980). An Introduction to Information Theory: Symbols, Signals and Noise. (Second revised edition). Dover Books.\nThe logarithm to the base 2 of a number is the power to which 2 must be raised to equal the number. We can express the same thing mathematically:\n\\[\n2^{\\log_2 x} = x\\\\\n\\]\nWith our three tosses of heads and tails:\n\\[\n\\log_2 8 = 3 \\\\\n2^{\\log_2 8} = 8 \\\\\n2^3 = 8\n\\]\nTwo must be raised to \\(3\\) to give us 8.\nPROMPT: What are the logarithms of the following numbers: 1, 2, 4, 8, 16, 32, 64?\n\n\n\n\\(\\mathbf{x}\\)\n\\(\\mathbf{\\log_2 x}\\)\nwhy?\n\n\n\n\n\\(1\\)\n\\(0\\)\n\\(2^0 = 1\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(2^1 = 2\\)\n\n\n\\(4\\)\n\\(2\\)\n\\(2^2 = 4\\)\n\n\n\\(8\\)\n\\(3\\)\n\\(2^3 = 8\\)\n\n\n\\(16\\)\n\\(4\\)\n\\(2^4 = 16\\)\n\n\n\\(32\\)\n\\(5\\)\n\\(2^5 = 32\\)\n\n\n\\(64\\)\n\\(6\\)\n\\(2^6 = 64\\)\n\n\n\nAs we can see when plotting the logarithms, the logarithm of a number \\(x\\) first grows very quickly, but then slows down.\n\n\nCode Toggle\nplot_log()\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Bits for Probabilities\nSo far we have worked with counts, but we started out asking how we could encode probabilities. How could we make that leap? An interesting property with the logarithm, is that taking the log of a fraction gives us the same result as taking the log of the denominator but negative. So\n\\[\n\\log_2 2 = 1\n\\]\nAnd:\n\\[\n\\log_2\\frac{1}{2} = - 1\n\\]\nThis means that counting logs for a uniform distribution where all events are equally probable is just like working with counts, but negative!\nPROMPT: What are the logarithms of the following numbers: \\(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}\\)\n\n\n\n\n\n\n\n\n\\(\\mathbf{x}\\)\n\\(\\mathbf{\\log_2 x}\\)\nwhy?\n\n\n\n\n\\(1\\)\n\\(0\\)\n\\(2^0 = 1\\)\n\n\n\\(\\frac{1}{2}\\)\n\\(-1\\)\n\\(2^{-1} = \\frac{1}{2}\\)\n\n\n\\(\\frac{1}{4}\\)\n\\(-2\\)\n\\(2^{-2} = \\frac{1}{4}\\)\n\n\n\\(\\frac{1}{8}\\)\n\\(-3\\)\n\\(2^{-3} = \\frac{1}{8}\\)\n\n\n\\(\\frac{1}{16}\\)\n\\(-4\\)\n\\(2^{-4} = \\frac{1}{16}\\)\n\n\n\\(\\frac{1}{32}\\)\n\\(-5\\)\n\\(2^{-5} = \\frac{1}{32}\\)\n\n\n\\(\\frac{1}{64}\\)\n\\(-6\\)\n\\(2^{-6} = \\frac{1}{64}\\)\n\n\n\nPlotting the log for our fractional \\(x\\) values, we can note that the log grows very quickly as we start from very small values and move to larger ones, only to flatten out asymptotically when we approach \\(x\\).\n\n\nCode Toggle\nplot_log(frac=True)\n\n\n\n\n\n\n\n\n\nOkay, given that we can use logarithms to count bits for probabilities, we are ready to move on to a more general definition of information that does not assume uniformity.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#bits-for-probabilities",
    "href": "notebooks/01_bits.html#bits-for-probabilities",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "2.4 Bits for Probabilities",
    "text": "2.4 Bits for Probabilities\nSo far we have worked with counts, but we started out asking how we could encode probabilities. How could we make that leap? An interesting property with the logarithm, is that taking the log of a fraction gives us the same result as taking the log of the denominator but negative. So\n\\[\n\\log_2 2 = 1\n\\]\nAnd:\n\\[\n\\log_2\\frac{1}{2} = - 1\n\\]\nThis means that counting logs for a uniform distribution where all events are equally probable is just like working with counts, but negative!\nPROMPT: What are the logarithms of the following numbers: \\(\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{8}, \\frac{1}{16}, \\frac{1}{32}, \\frac{1}{64}\\)\n\n\n\n\n\n\n\n\n\\(\\mathbf{x}\\)\n\\(\\mathbf{\\log_2 x}\\)\nwhy?\n\n\n\n\n\\(1\\)\n\\(0\\)\n\\(2^0 = 1\\)\n\n\n\\(\\frac{1}{2}\\)\n\\(-1\\)\n\\(2^{-1} = \\frac{1}{2}\\)\n\n\n\\(\\frac{1}{4}\\)\n\\(-2\\)\n\\(2^{-2} = \\frac{1}{4}\\)\n\n\n\\(\\frac{1}{8}\\)\n\\(-3\\)\n\\(2^{-3} = \\frac{1}{8}\\)\n\n\n\\(\\frac{1}{16}\\)\n\\(-4\\)\n\\(2^{-4} = \\frac{1}{16}\\)\n\n\n\\(\\frac{1}{32}\\)\n\\(-5\\)\n\\(2^{-5} = \\frac{1}{32}\\)\n\n\n\\(\\frac{1}{64}\\)\n\\(-6\\)\n\\(2^{-6} = \\frac{1}{64}\\)\n\n\n\nPlotting the log for our fractional \\(x\\) values, we can note that the log grows very quickly as we start from very small values and move to larger ones, only to flatten out asymptotically when we approach \\(x\\).\n\n\nCode Toggle\nplot_log(frac=True)\n\n\n\n\n\n\n\n\n\nOkay, given that we can use logarithms to count bits for probabilities, we are ready to move on to a more general definition of information that does not assume uniformity.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  },
  {
    "objectID": "notebooks/02_entropy.html",
    "href": "notebooks/02_entropy.html",
    "title": "3¬† Entropy: Expected Differences",
    "section": "",
    "text": "The Big Question: How do we most effectively code messages as signals?\nWe are now ready to understand the mathematical concept of information entropy.\nTo do so, let‚Äôs return to the unbiased coin, once again. We already know we need two bits to encode it. BUt what if we wanted to calculate this value just from knowing the probabilities:\n\\[\nP(H) = \\frac{1}{2} \\;\\;\\;\nP(T) = \\frac{1}{2}\n\\]\nIf we take the log of these, we get:\n\\[\n\\log_2\\frac{1}{2} + \\log_2\\frac{1}{2} = \\\\\n-1 + (-1) = \\\\\n-2\n\\]\nBut that‚Äôs too much! And negative! We know the bits to encode a fair coin is one. What we need to do is to weigh each event in our distribution by it‚Äôs probability to get not just a sum but a ‚Äúweighted sum‚Äù. \\[\n\\begin{equation*}\n\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) = \\\\\n\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right) = \\\\\n-\\frac{1}{2} -\\frac{1}{2} = \\\\\n-1\n\\end{equation*}\n\\]\nBetter! But still negative. What we do is just reverse the sign of the equation, like this:\n\\[\n\\begin{equation*}\n-1 \\times \\left[\\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right) + \\left(\\frac{1}{2}\\times\\log_2\\frac{1}{2}\\right)\\right] = \\\\\n-1 \\times \\left[\\left(\\frac{1}{2}\\times(-1)\\right) + \\left(\\frac{1}{2}\\times(-1)\\right)\\right] = \\\\\n-1 \\times \\left[-\\frac{1}{2} -\\frac{1}{2}\\right] = \\\\\n-1 \\times (-1) = \\\\\n1\n\\end{equation*}\n\\]\nWe can actually use this to take a stab at a first definition of information entropy. We will use the symbol \\(\\mathbb{H}\\). So let‚Äôs imagine a distribution \\(X\\) with two outcomes \\(x_1\\) and \\(x_2\\) and let‚Äôs for simplicity‚Äôs sake say they have probabilities \\(p_1\\) and \\(p_2\\). Then, the information entropy is:\n\\[\n\\mathbb{H}(X) = -(p_1 \\log_2 p_1 + p_2 \\log_2 p_2))\n\\]\nAnd we can actually generalize this to a case with any number of outcomes and with any probability values, like this:\n\\[\n\\begin{equation}\n  \\mathbb{H}(X) = -\\overbrace{\\sum_{i=1}^N P(x_i)}^\\text{Weighted sum} \\underbrace{\\log_2 P(x_i)}_\\text{Bits}\n\\end{equation}\n\\]\nLet‚Äôs unpack again.\nPROMPT: How would you code entropy if you were to use a for loop? Hint: You will need to use np.log2\nGreat! Let‚Äôs wrap that in a function. \\(\\log_2 0\\) is undefined, so we need to handle that separately.\nLet‚Äôs now go over all these distributions and get the entropy for each of them. We‚Äôll need a list of distributions.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Entropy: Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/02_entropy.html#bits-for-a-very-simple-language",
    "href": "notebooks/02_entropy.html#bits-for-a-very-simple-language",
    "title": "3¬† Entropy: Expected Differences",
    "section": "3.1 Bits for (a very simple) Language",
    "text": "3.1 Bits for (a very simple) Language\nLet‚Äôs now imagine an artificial language with only four letters, all equally probable. The letters are our unknown \\(x_i\\) variable, and we index them with \\(i\\) like this:\n\\[\nx_1 = A, \\; x_2 = B, \\; x_3 = C, \\; x_4 = D\n\\]\nThey also have corresponding probabilities, so that\n\\[\np(x_1) = 0.25, \\; p(x_2) = 0.25, \\; p(x_3) = 0.25, \\; p(x_4) = 0.25\n\\]\n\n\nCode Toggle\nlanguage = ['A', 'B', 'C', 'D']\nuniform_probs = [0.25]*4\nuniform_probs\n\n\n[0.25, 0.25, 0.25, 0.25]\n\n\nLet‚Äôs plot the language as a distribution\n\n\nCode Toggle\ndef plot_symbol_probs(uniform_probs, language):\n    fig, ax = plt.subplots(figsize=(5.8, 3.8))\n    \n    # histogram-style bars (outline only, no fill)\n    ax.bar(\n        range(len(uniform_probs)), \n        uniform_probs, \n        edgecolor=\"black\", \n        color=\"lightblue\",\n        fill=True, \n        linewidth=1.5\n    )\n    \n    ax.set_xticks(range(len(language)))\n    ax.set_xticklabels(language, fontsize=12)\n    ax.set_ylabel(\"Probability\", fontsize=14)\n    ax.set_xlabel(\"Language\", fontsize=14)\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=12)\n    ax.grid(True, linestyle='-', linewidth=0.5, alpha=0.6)\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode Toggle\nplot_symbol_probs(uniform_probs, language)\n\n\n\n\n\n\n\n\n\nOur language has a uniform distribution, so the line is flat. We‚Äôve seen this type of event before: - When we were tossing our fair coin two times, we had four possible outcomes, just like now. - The average bits we needed to encode those outcomes were \\(2\\).\nLet‚Äôs see if that‚Äôs true now as well:\n\n\nCode Toggle\nentropy_np(uniform_probs).item()\n\n\n2.0\n\n\nNo surprises here. Or, from Shannon‚Äôs perspective, only surprises! - Each outcome requires two bits to encode. - To clarify, we can re-write our code for the bit table to include the probability of the events.\n\n\nCode Toggle\ndef print_bit_table(outcome, bits, probs):\n    n_bits = [len(b) for b in bits]\n    \n    return pd.DataFrame(\n        {'outcomes':outcome, \n         'bits':bits, \n         'n_bits':n_bits,\n         'prob':np.round(probs, 3).astype(str)\n        }).style.hide()\n\n\n\n\nCode Toggle\nuniform_bits = ['00', '01', '10', '11']\nprint_bit_table(language, uniform_bits, uniform_probs)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\nprob\n\n\n\n\nA\n00\n2\n0.25\n\n\nB\n01\n2\n0.25\n\n\nC\n10\n2\n0.25\n\n\nD\n11\n2\n0.25\n\n\n\n\n\nBut what if our language instead had the following probabilities:\n\\[\n\\begin{aligned}\np(x_1) &= 0.5 \\\\\np(x_2) &= 0.25 \\\\\np(x_3) &= 0.125 \\\\\np(x_4) &= 0.125\n\\end{aligned}\n\\]\n\n\nCode Toggle\nlanguage = ['A', 'B', 'C', 'D']\nlang_probs = [0.5, 0.25, 0.125, 0.125]\nsum(lang_probs)\n\n\n1.0\n\n\nLet‚Äôs plot that again.\n\n\nCode Toggle\nplot_symbol_probs(lang_probs, language)\n\n\n\n\n\n\n\n\n\nLooks familiar? Well, we‚Äôve just created a language that seems to follow Zipf‚Äôs Law. It‚Äôs more simple than an actual language to make analysis easier, but the idea is the same:\n\n\nCode Toggle\nimport nltk\n\nfrom nltk.corpus import gutenberg\nfrom nltk.tokenize import RegexpTokenizer\n\n\n\n\nCode Toggle\ndef tokens_from_gutenberg_text(txt_name):\n    sents = gutenberg.sents(txt_name)\n    words = [word for sent in sents for word in sent]\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(\" \".join(words).lower())\n    return tokens\n\n\n\n\nCode Toggle\ndef get_probs_from_counter(counts):\n    sum_ = sum(counts.values())\n    probs = Counter()\n    for token in counts:\n        probs[token] = np.float32(np.format_float_positional(counts[token] / sum_))\n    return probs\n\n\n\n\nCode Toggle\nalice_tokens = tokens_from_gutenberg_text('carroll-alice.txt')\nalice_counts = Counter(alice_tokens)\nalice_probs = get_probs_from_counter(alice_counts)\n\n\n\n\nCode Toggle\ndef plot_zipf_dist(counts, p_type='bar'):\n    zipf_df = (\n        pd.DataFrame.from_dict(dict(counts.most_common(18)), orient='index')\n        .reset_index()\n        .rename(columns={'index': 'Word', 0: 'Count'})\n    )\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    plt.xticks(rotation=70)\n\n    if p_type == 'bar':\n        ax.bar(\n            zipf_df[\"Word\"],\n            zipf_df[\"Count\"],\n            edgecolor=\"black\",\n            color=\"lightblue\",\n            fill=True,\n            linewidth=1.5\n        )\n    elif p_type == 'line':\n        ax.plot(\n            zipf_df[\"Word\"],\n            zipf_df[\"Count\"],\n            color=\"blue\",\n            linewidth=1.5,\n            marker=\"o\",\n            markersize=4,\n            markerfacecolor=\"white\"\n        )\n\n    # ticks and labels (kept same behavior)\n    _, xlabels = plt.xticks()\n    _, ylabels = plt.yticks()\n    ax.set_xticklabels(xlabels, size=12)\n    ax.set_yticklabels(ylabels, size=12)\n\n    # axis labels\n    ax.set_xlabel(\"Word\", fontsize=15)\n    ax.set_ylabel(\"Probability\", fontsize=15)\n\n    # grid and layout style (consistent with your other figures)\n    ax.grid(True, linestyle='-', linewidth=0.5, alpha=0.6)\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode Toggle\nplot_zipf_dist(alice_probs)\n\n\n/var/folders/zz/3lhn60g1163cfzw5vrc_yrp00000gn/T/ipykernel_81698/4135042961.py:34: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_xticklabels(xlabels, size=12)\n/var/folders/zz/3lhn60g1163cfzw5vrc_yrp00000gn/T/ipykernel_81698/4135042961.py:35: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n  ax.set_yticklabels(ylabels, size=12)\n\n\n\n\n\n\n\n\n\nIf we now want to encode this language in bits, how do we go about it? Well, for starters we know how many bits we need on average. That knowledge is provided by our entropy function.\n\n\nCode Toggle\nlang_entropy = entropy_np(lang_probs)\nlang_entropy.item()\n\n\n1.75\n\n\nBut we know more than that ‚Äî each outcome needs a number of bits that corresponds to its probability:\n\\[\n\\begin{aligned}\n\\text{bits for } A &= -\\log p(A) = -\\log\\frac{1}{2} = 1 \\\\\n\\text{bits for } B &= -\\log p(B) = -\\log\\frac{1}{4} = 2 \\\\\n\\text{bits for } C &= -\\log p(C) = -\\log\\frac{1}{8} = 3 \\\\\n\\text{bits for } D &= -\\log p(D) = -\\log\\frac{1}{8} = 3\n\\end{aligned}\n\\]\nDoes this work? Let‚Äôs try!\nPROMPT: Calculate how many bits we have on average, if we weigh each by their corresponding probability.\n\n\nCode Toggle\naverage_bits = (0.5 * 1 + 0.25 * 2 + 0.125*3 + 0.125*3)\naverage_bits\n\n\n1.75\n\n\nIt matches the entropy! Let‚Äôs make that into a function.\n\n\nCode Toggle\ndef average_bits(probs, n_bits):\n    return np.sum(np.multiply(probs, n_bits))\n\nn_lang_bits = [1,2,3,3]\n(average_bits(lang_probs, n_lang_bits) == entropy_np(lang_probs)).item()\n\n\nTrue\n\n\nIf we output our bit table with these values, we see what such an encoding could look like.\nPROMPT Create the list lang_bits with four entries. Make these correspond to the bits each letter should have:\n\\[\n\\begin{aligned}\nA & : 1 \\\\\nB & : 2 \\\\\nC & : 3 \\\\\nD & : 3\n\\end{aligned}\n\\]\n\n\nCode Toggle\nlang_bits = ['0', '10', '110', '111']\nprint_bit_table(language, lang_bits, lang_probs)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\nprob\n\n\n\n\nA\n0\n1\n0.5\n\n\nB\n10\n2\n0.25\n\n\nC\n110\n3\n0.125\n\n\nD\n111\n3\n0.125",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Entropy: Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/03_kl_divergence.html",
    "href": "notebooks/03_kl_divergence.html",
    "title": "4¬† KL Divergence: Differences Between Expected Differences",
    "section": "",
    "text": "4.1 Comparing Distributions: Relative Entropy & Redundancy\nLet‚Äôs reiterate what we‚Äôve learned so far.\nFirst, we‚Äôve learned to calculate:\nSecond, we know that:\nBut what if we want to compare distributions? What if we want to somehow measure the entropy of their difference?\nLet‚Äôs start with two familiar examples. - We have our language with \\(A\\), \\(B\\), \\(C\\) and \\(D\\). - We know their probabilities. - We also know what it would look like if we assumed an uniform distribution for them.\nLet‚Äôs now compare these two distributions.\nCode Toggle\ndef compare_distributions_in_plot(outcomes,\n                                  first_dist, \n                                  second_dist,\n                                  first_label,\n                                  second_label,\n                                  xlabel=\"Outcomes\",\n                                  ylabel=\"Probability\",\n                                  title=\"Two distributions\"):\n    \n    x = np.arange(len(outcomes))\n    width = 0.35  # spacing between the two bars\n\n    fig, ax = plt.subplots(figsize=(10,7))\n\n    # outline-only bars for the clean style\n    ax.bar(x - width/2, first_dist, width,\n           edgecolor=\"black\", color=\"lightblue\", fill=True, linewidth=1.5, label=first_label)\n    ax.bar(x + width/2, second_dist, width,\n           edgecolor=\"black\", color=\"orange\", fill=True, linewidth=1.5, label=second_label)\n\n    ax.set_xlabel(xlabel, fontsize=16)\n    ax.set_ylabel(ylabel, fontsize=16)\n    ax.set_xticks(x)\n    ax.set_xticklabels(outcomes)\n    ax.tick_params(axis='both', which='major', labelsize=14)\n    ax.grid(True, linestyle='-', linewidth=0.5, alpha=0.6)\n    ax.legend(fontsize=14)\n    ax.set_title(title, fontsize=16)\n    plt.tight_layout()\n    plt.show()\nCode Toggle\nlanguage = ['A', 'B', 'C', 'D']\nuniform_probs = [0.25]*4\nlang_probs = [0.5, 0.25, 0.125, 0.125]\nCode Toggle\ncompare_distributions_in_plot(language, \n                              lang_probs, \n                              uniform_probs,\n                              'Actual',\n                              'Uniform',\n                              'Outcomes',\n                              'Probability',\n                              'Comparing the actual distribution of our language to a uniform distribution')\nOne way we could compare them, is to just take their ratio. We will call this ratio ‚Äúrelative entropy‚Äù:\n\\[\n\\text{Relative entropy} = \\frac{\\text{Entropy of a distribution}}{\\text{The max entropy of the same distribution}}\n\\]\nPROMPT: Can you write that in Python for our example above? Wrap it in a function like this: def relative_entropy(true_ent, max_ent).\nRecall the entropies of these two distributions:\nCode Toggle\ndef entropy_np(p):\n    p = np.array(list(p))\n    p = p[p &gt; 0]\n    return -np.sum(p * np.log2(p))\nCode Toggle\nuniform_entropy = entropy_np(uniform_probs)\nuniform_entropy.item()\n\n\n2.0\nCode Toggle\nlang_entropy = entropy_np(lang_probs)\nlang_entropy.item()\n\n\n1.75\nCode Toggle\ndef relative_entropy(true_ent, max_ent):\n    return true_ent / max_ent\nCode Toggle\nrelative_entropy(lang_entropy, uniform_entropy).item()\n\n\n0.875\nShannon introduces this concept in his work to describe the difference between the entropy of our source and a completely uniform version of the same source. In Shannon‚Äôs (p.¬†56) own words:\nWhat does relative entropy capture then? Weaver gets into this question in his intro to Shannon‚Äôs work:\nShannon himself did a bunch of experiments to conclude that the redundancy of written English is about 50%. In his opinion:\nOne way to better understand what relative entropy captures, is by looking at it‚Äôs relationship to redundancy. Shannon defined redundancy in terms of relative entropy as:\n\\[\n\\text{Redundancy }= 1-\\text{ Relative Entropy}\n\\]\nRelative entropy tells us how much freedom we have and redundancy tells us how much we lack. The uniform distribution is total freedom: Everything is equally possible! Once we add structure, our choices become more constrained. According to Shannon, redundancy\nThere are some interesting philosophical implications here: Our use of language is just the actualization of the virtual space of possible outcomes, governed by statistical rules. A decisively posthuman vision of communication!\nPROMPT: What is the redundancy of our artificial language?\nIf we go back to our actual data, we can see that our language has 0.875 relative entropy and 0.125 redundancy.\nCode Toggle\nrelative_entropy(lang_entropy, uniform_entropy).item()\n\n\n0.875\nCode Toggle\n1-relative_entropy(lang_entropy, uniform_entropy).item()\n\n\n0.125\nWhile Shannon and Weaver limited their analysis of relative entropy to the ratio between the entropy of a source and its max entropy, the principle of calculating such ratios can be extended.\nLet‚Äôs now instead say that we are familiar with the language \\(A\\), \\(B\\), \\(C\\) \\(D\\), but we don‚Äôt quite know what the probabilities for each outcome are. Based on our prior knowledge, we would guess it is something like this:\n\\[\n\\begin{aligned}\np(A) &= 0.625 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.125 \\\\\np(D) &= 0.125\n\\end{aligned}\n\\]\nCode Toggle\nassumed_probs = [0.625, 0.125, 0.125, 0.125]\nsum(assumed_probs)\n\n\n1.0\nComparing these visually, we see that it‚Äôs an okay estimation.\nCode Toggle\ncompare_distributions_in_plot(language, \n                              lang_probs, \n                              assumed_probs,\n                              'Actual',\n                              'Assumed',\n                              'Outcomes',\n                              'Probability',\n                              'Comparing the actual distribution of our language to our assumed distribution')\nThe entropy for our estimate is:\nCode Toggle\nassumed_entropy = entropy_np(assumed_probs)\nassumed_entropy\n\n\nnp.float64(1.5487949406953985)\nAnd compared to you actual entropy, we see that our assumed language is actually more structured. The high probability given to \\(A\\) makes it more deterministic:\nCode Toggle\nrelative_entropy(lang_entropy, assumed_entropy)\n\n\nnp.float64(1.1299107157557358)\nLet‚Äôs stop for a moment to recall what we set out to do: We wanted to find a metric to measure the difference between different probability distributions. And now we‚Äôve done?!\nWell yes, kind of. But there‚Äôs a problem.\nPROMPT: What might isse be with using relative entropy to compare distributions?\nWell, the problem is that relative entropy doesn‚Äôt account for the individual probabilities of our outcomes. What kind of issues might this create? Let‚Äôs clarify with an example.\nImagine our language from above, but with inverse probabilities. So:\n\\[\n\\begin{aligned}\np(A) &= 0.125 \\\\\np(B) &= 0.125 \\\\\np(C) &= 0.25  \\\\\np(D) &= 0.5\n\\end{aligned}\n\\]\nCode Toggle\ninverse_probs = lang_probs.copy()\ninverse_probs.reverse()\ninverse_probs\n\n\n[0.125, 0.125, 0.25, 0.5]\nNow we see that while our distributions have shaped that mirror each other, our estimate would surely produce terrible predictions!\nCode Toggle\ncompare_distributions_in_plot(language, \n                              lang_probs, \n                              inverse_probs,\n                              'Actual',\n                              'Inverse',\n                              'Outcomes',\n                              'Probability',\n                              'Comparing the actual distribution of our language to a inverse distribution')\nPROMPT: What is the relative entropy between these two distributions?\nCode Toggle\nreversed_entropy = entropy_np(inverse_probs)\nreversed_entropy\n\n\nnp.float64(1.75)\nCode Toggle\nrelative_entropy(lang_entropy, reversed_entropy)\n\n\nnp.float64(1.0)\nCode Toggle\nrelative_entropy(lang_entropy, lang_entropy)\n\n\nnp.float64(1.0)\nRelative entropy says nothing about the actual shape of the distribution, it just tells what it‚Äôs mean is. We will need sharper tools to actually compare distributions. But we will still use entropy!",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>KL Divergence: Differences Between Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/03_kl_divergence.html#comparing-distributions-relative-entropy-redundancy",
    "href": "notebooks/03_kl_divergence.html#comparing-distributions-relative-entropy-redundancy",
    "title": "4¬† KL Divergence: Differences Between Expected Differences",
    "section": "",
    "text": "The ratio of the entropy of a source to the maximum value it could have while still restricted to the same symbols will be called its relative entropy.\n\n\n\nIf the relative entropy of a certain source is, say .8, this roughly means that this source is, in its choice of symbols to form a message, about 80 per cent as free as it could possibly be with these same symbols.\n\n\n\nThis means that when we write English half of what we write is determined by the structure of the language and half is chosen freely.\n\n\n\n\n\nis the fraction of the structure of the message which is determined not by the free choice of the sender, but rather by the accepted statistical rules governing the use of the symbols in question.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>KL Divergence: Differences Between Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/03_kl_divergence.html#comparing-distributions-the-kullback-leibler-divergence",
    "href": "notebooks/03_kl_divergence.html#comparing-distributions-the-kullback-leibler-divergence",
    "title": "4¬† KL Divergence: Differences Between Expected Differences",
    "section": "4.2 Comparing Distributions: The Kullback-Leibler Divergence",
    "text": "4.2 Comparing Distributions: The Kullback-Leibler Divergence\nToday, no one uses relative entropy or redundancy in machine learning, at least not in the sense that they were defined by Shannon. Instead, people use metrics that were further developed from the idea of relative entropy by other people who built on Shannon‚Äôs work.\nOne particularly important measure like this is the Kullback-Leibler Divergence or \\(\\mathbb{KL}\\). - It was developed by mathemtaicians Solomon Kullback and Richard Leibler in a 1951 paper. - While they didn‚Äôt name it after themselves, other people since then have started using this name. - It‚Äôs from the \\(\\mathbb{KL}\\) divergence that cross-entrop, the perhaps most commonly used tool for comparing distributions in ML, is derived.\nLet‚Äôs start with intuition again.\nWhat if for every outcome \\(x_i\\) in our distribution, we compared the number of bits we need to encode that outcome?\nPROMPT: Can you code such a for-loop in Python?\n\n\nCode Toggle\nentropy_diff = 0\n\nfor p in lang_probs:\n    entropy_diff += (np.log2(p) - np.log2(0.25))\nentropy_diff\n\n\nnp.float64(-1.0)\n\n\nWhat? We actually need in total one bit more to encode our structured language than we need for the uniform distribution. And indeed, this is true, we see it from our bit tables:\n\n\nCode Toggle\ndef print_bit_table(outcome, bits, probs):\n    n_bits = [len(b) for b in bits]\n    \n    return pd.DataFrame(\n        {'outcomes':outcome, \n         'bits':bits, \n         'n_bits':n_bits,\n         'prob':np.round(probs, 3).astype(str)\n        }).style.hide()\n\n\n\n\nCode Toggle\nlang_bits = ['0', '10', '110', '111']\nprint_bit_table(language, lang_bits, lang_probs)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\nprob\n\n\n\n\nA\n0\n1\n0.5\n\n\nB\n10\n2\n0.25\n\n\nC\n110\n3\n0.125\n\n\nD\n111\n3\n0.125\n\n\n\n\n\n\n\nCode Toggle\nuniform_bits = ['00', '01', '10', '11']\nprint_bit_table(language, uniform_bits, uniform_probs)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\nprob\n\n\n\n\nA\n00\n2\n0.25\n\n\nB\n01\n2\n0.25\n\n\nC\n10\n2\n0.25\n\n\nD\n11\n2\n0.25\n\n\n\n\n\n\n\nCode Toggle\nprint_bit_table(language, uniform_bits, uniform_probs)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\nprob\n\n\n\n\nA\n00\n2\n0.25\n\n\nB\n01\n2\n0.25\n\n\nC\n10\n2\n0.25\n\n\nD\n11\n2\n0.25\n\n\n\n\n\nOur structured language requires 9 bits to encode, our uniform distribution only takes 8:\n\n\nCode Toggle\nn_lang_bits = [1,2,3,3]\n\n\n\n\nCode Toggle\nprint(sum(n_lang_bits))\nprint(4*2)\n\n\n9\n8\n\n\nBut entropy isn‚Äôt about sums, it‚Äôs about weighted sums, also known as ‚Äúexpected value‚Äù or, simply, ‚Äúmean‚Äù.\nSo if we rewrite our loop, but now weigh every difference. What should we weigh it by? - Well, how about the probabilities of the actual distribution we are interested in? - Weighing the loop with the probabilities of our language, we get:\n\n\nCode Toggle\nentropy_diff = 0\n\nfor p in lang_probs:\n    entropy_diff += p*(np.log2(p) - np.log2(0.25))\nentropy_diff\n\n\nnp.float64(0.25)\n\n\nWhat we now get is the distance of the uniform distribution from the vantage point of our the probabilities of our artificial language. And this is the \\(\\mathbb{KL}\\) divergence! That‚Äôs all there is to it. Let‚Äôs write it in numpy and try it out.\nPROMPT: Write the \\(\\mathbb{KL}\\) divergence in numpy.\nHint: Here you have it as a loop:\n\n\nCode Toggle\ndef kl_divergence_loop(p, q):\n    kld = 0\n    for idx, p in enumerate(lang_probs):\n        kld += p*(np.log2(p) - np.log2(q[idx]))\n    return kld\n\n\n\n\nCode Toggle\ndef kl_divergence(p, q):\n    return np.sum(p * np.log2(p / q), 0)\n\n\nNice! Let‚Äôs try it out:\nOur uniform distribution is on average 0.25 bits away from our actual distribution.\n\n\nCode Toggle\nkl_divergence(np.array(lang_probs), np.array(uniform_probs))\n\n\nnp.float64(0.25)\n\n\nWhy is this true? Because, as we saw above, their total distance is \\(1\\), so with four outcomes the average distance is \\(\\frac{1}{4} = 0.25\\).\nOur estimate for the language is already a lot closer:\n\n\nCode Toggle\nkl_divergence(np.array(lang_probs), np.array(assumed_probs))\n\n\nnp.float64(0.08903595255631885)\n\n\nThen, if we compare our language to itself, the distance is \\(0\\):\nThis means that we know exactly what distribution \\(P\\) generated the data and the difference in bits needed to encode the distributions is zero. In other words,\n\nit means that we can correctly predict the probabilities of all possible future events, and thus we have learned to predict the future as well as an ‚Äôoracle‚Äô that has access to the true distribution P (Murphy 2021, 243).\n\nFinally, how does the \\(\\mathbb{KL}\\) divergence do on our inverse distribution?\n\n\nCode Toggle\nkl_divergence(np.array(lang_probs), np.array(inverse_probs))\n\n\nnp.float64(0.875)\n\n\nQuite well, it turns out! Whereas relative entropy was unable to distinguish them, \\(\\mathbb{KL}\\) divergence shows that they are further away from each other than any other distributions we compared.\n\\[\\begin{aligned}\n\\mathbb{KL}(p \\Vert q)\n    &= \\overbrace{E\\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big]}^{\\color{red}{\\text{Expected surprise of p when encoding with q}}} \\\\\n    &= \\sum_i p(x_i) \\big[\\log_2 p(x_i) - \\log_2 q(x_i)\\big] \\\\\n    &= \\sum_i p(x_i) \\log_2 \\frac{p(x_i)}{q(x_i)} \\\\\n    &= \\sum_i p(x_i)\\log_2 p(x_i) - \\sum_i p(x_i)\\log_2 q(x_i) \\\\\n    &= \\underbrace{-\\mathbb{H}(p)}_{\\text{Negentropy of p}} + \\underbrace{\\mathbb{H}(p,q)}_{\\text{Cross-entropy between p and q}}\n\\end{aligned}\\]\nLooks hard? Well, sure. But mostly because we aren‚Äôt familiar with the notation and the associated rules of different symbols. Building up the intuition slowly and in code, it‚Äôs hopefully more clear :)\nNow we can move on to our last part: cross-entropy. This function is the last terms of the \\(\\mathbb{KL}\\) divergence above and probably the most commonly used optimization function in neural networks today.",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>KL Divergence: Differences Between Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/03_kl_divergence.html#comparing-distributions-cross-entropy",
    "href": "notebooks/03_kl_divergence.html#comparing-distributions-cross-entropy",
    "title": "4¬† KL Divergence: Differences Between Expected Differences",
    "section": "4.3 Comparing Distributions: Cross-Entropy",
    "text": "4.3 Comparing Distributions: Cross-Entropy\nWith cross-entropy, we take a step back and go to the original definition of entropy. - What if we encoded our artificial language with bits corresponding to some other language. - For example: What if we used the bits for the uniform distribution to encode the artificial language? - How many bits would we then need on average.\nLet‚Äôs say the distributions \\(P\\) and \\(Q\\) have the same outcomes \\(x_i\\), but with different probabilities \\(p(x_i)\\) and \\(q(x_i)\\). Then, in terms of our equation for information, it would look like this:\n\\[\n\\begin{equation}\n  \\mathbb{H}(P,Q) = -\\overbrace{\\sum_{i=1}^N p(x_i)}^\\text{Weighted sum for P} \\underbrace{\\log_2 q(x_i)}_\\text{Bits for Q}\n\\end{equation}\n\\]\nWe can modify our \\(\\mathbb{KL}\\) divergence loop accordingly.\nPROMPT: Write cross-entropy with a for-loop or in numpy.\n\nHint: You can use both the for-loop and numpyimplementations of \\(\\mathbb{KL}\\) divergence and the equation above.\n\n\nCode Toggle\ndef cross_entropy_loop(p, q):\n    ce = 0\n    for idx, p in enumerate(lang_probs):\n        ce += p*(- np.log2(q[idx]))\n    return ce\n\n\n\n\nCode Toggle\ncross_entropy_loop(lang_probs, uniform_probs)\n\n\nnp.float64(2.0)\n\n\n\n\nCode Toggle\ndef cross_entropy_np(p, q):\n    return -np.sum(p * np.log2(q))\n\n\n\n\nCode Toggle\ncross_entropy_np(lang_probs, uniform_probs)\n\n\nnp.float64(2.0)\n\n\nWhat do these numbers tell us? Well, just how many bits we need on average to encode our artificial language if we instead assume it is uniform. For our estimate, it‚Äôs already lower:\ncross_entropy_np(lang_probs, assumed_probs)\nBut the most efficient encoding is achieved using the actual distribution of the langauge itself. And, indeed, the cross-entropy between our language and itself is just the entropy of the language! Like plain old entropy, it tells us how many bits we need on average to code the language with it‚Äôs own encoding.\n\n\nCode Toggle\ncross_entropy_np(np.array(lang_probs), np.array(lang_probs))\n\n\nnp.float64(1.75)\n\n\nAnd that‚Äôs it! You now know as much if not a lot more information theory as most people doing machine learning. Most people don‚Äôt really understand cross-entropy, they just use it :-S",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>KL Divergence: Differences Between Expected Differences</span>"
    ]
  },
  {
    "objectID": "notebooks/01_bits.html#the-most-simple-language",
    "href": "notebooks/01_bits.html#the-most-simple-language",
    "title": "2¬† Bits: The Difference that Makes a Difference",
    "section": "2.5 The most simple language",
    "text": "2.5 The most simple language\nPROMPT: If we want to represent a coin toss as bits, how do we proceed?\n\n\nCode Toggle\ndef print_simple_bit_table(outcome, bits):\n    n_bits = [len(b) for b in bits]\n    \n    return pd.DataFrame(\n        {'outcomes':outcome, \n         'bits':bits, \n         'n_bits':n_bits}).style.hide()\n\n\n\n\nCode Toggle\nprint_simple_bit_table(['heads', 'tails'], ['0', '1'])\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nheads\n0\n1\n\n\ntails\n1\n1\n\n\n\n\n\n\n\nCode Toggle\nprobs = [0.5, 0.5]\nn_bits = [1, 1]\n\n\nPROMPT: Let‚Äôs move up. If we want to represent the sample space of two coin tosses in bits, how do we do it?\nPROMPT: What is the sample space?\n\\(A = \\{HH, HT, TT, TH \\}\\)\n\n\nCode Toggle\ntwo_toss_combinations = ['HH', 'HT', 'TT', 'TH']\ntwo_bit_combinations = ['00', '01', '10', '11']\n\nprint_simple_bit_table(two_toss_combinations, two_bit_combinations)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nHH\n00\n2\n\n\nHT\n01\n2\n\n\nTT\n10\n2\n\n\nTH\n11\n2\n\n\n\n\n\n\n\nPROMPT: If someone did this experiment, how many questions would we need on average to find out what the outcome was?\n\n Two questions!\nFirst question: Is the first throw heads? If yes, we have \\(\\{HH, HT\\}\\) left. If no, we have \\(\\{TT, TH \\}\\) left.\nSecond question: Is the first throw heads? Whatever the answer, only one option will remain.\nOf course we could get lucky by asking just one question. Say, for example, the outcome was \\(HH\\). If we ask ‚Äúwas it heads and heads‚Äù, we would‚Äôve needed only one question. But then we would‚Äôve been lucky. On average we need two questions. This was the big innovation from Shannon. A bit is the number of yes-no questions we need to ask in order to know an outcome in an experiment. This is why Gregory Bateson called a bit ‚Äúthe difference that makes a difference‚Äù.\n\nBefore we go on to formalize this discovery mathematically, let‚Äôs build up some more intuition.\nLet‚Äôs consider an experiment with three trials or tosses. Typing that out is annoying, so we‚Äôll use a helper function for it.\n\n\nCode Toggle\nfrom itertools import product\n\ndef produce_N_combinations(items, N):\n    return [''.join(x) for x in product(items, repeat = N)]\n\n\nPROMPT: How big is the sample space of eight trials? You can use produce_N_compitations to find out.\n\n\nCode Toggle\nthree_toss_combinations = produce_N_combinations(['T', 'H'], 3)\nthree_toss_combinations\n\n\n['TTT', 'TTH', 'THT', 'THH', 'HTT', 'HTH', 'HHT', 'HHH']\n\n\nPROMPT: How would we encode that in bits? Use produce_N_combinations again, if you want.\n\n\nCode Toggle\nthree_bit_combinations = produce_N_combinations(['0', '1'], 3)\nthree_bit_combinations\n\n\n['000', '001', '010', '011', '100', '101', '110', '111']\n\n\n\n\nCode Toggle\nprint_simple_bit_table(three_toss_combinations, \n                       three_bit_combinations)\n\n\n\n\n\n\n\noutcomes\nbits\nn_bits\n\n\n\n\nTTT\n000\n3\n\n\nTTH\n001\n3\n\n\nTHT\n010\n3\n\n\nTHH\n011\n3\n\n\nHTT\n100\n3\n\n\nHTH\n101\n3\n\n\nHHT\n110\n3\n\n\nHHH\n111\n3\n\n\n\n\n\n\n\nClick to execute code\n\n#| echo: true\n#| output: true\n\nimport numpy as np\nresult = np.log2(8)\nprint(result)\n\nPROMPT: How many questions do we need now?\nThree questions!\nFirst question: Is the first throw heads? If yes, we have \\(\\{HTT, HTH, HHT, HHH\\}\\) left. If no, we have \\(\\{TTT, TTH, THT, THH \\}\\) left.\nWith four options left, we know from above we need just two more questions. So three questions in total!\nPROMPT: Do you notice some sort of connection between how we build our outcomes and the number of bits we need to describe the distribution?\nYes, there is indeed a connection. And that‚Äôs good, because all this counting can get tiresome‚Ä¶\nTo know how many options we have to choose among, we can use a simple formula. If \\(S\\) is the number of symbols we can choose from (i.e.¬†2, because we choose between \\(H\\) and \\(T\\)) and \\(n\\) is the number of combinations of those (3 in the above example), then the number of basic outcomes \\(E\\) is:\n\\(E = S^n\\)\nLet‚Äôs try it out:\n\n\nCode Toggle\nS = 2\nn = 3\n\n\n\n\nCode Toggle\nE = np.power(S, n)\nE.item()\n\n\n8\n\n\nPROMPT: How many bits will we need to represent these options?\nIf we already know \\(E\\), we can find out how many bits we need by taking the logarithm of \\(E\\), because the logarithm is simply the inverse of a power:\n\n\nCode Toggle\nnp.log2(E)\n\n\nnp.float64(3.0)\n\n\nIn fact, we can count the number of bits we need directly from \\(n\\) and \\(S\\):\n\n\nCode Toggle\nn*np.log2(S)\n\n\nnp.float64(3.0)\n\n\nThis is also how Ralph Hartley, an engineer at Bell Labs, defined information in a paper in 1928. For him information was just:\n\n\nHartley, R. V. L. (1928). Transmission of Information. Bell System Technical Journal, 7(3), 535‚Äì563. [URL]\n\\[\nn \\cdot \\log S\n\\]\nBut this isn‚Äôt the definition of that history settled on, and only applies to uniform distributions. Before we move on, let‚Äôs take a little detour.\n\n2.5.1 Aside: What Does the Logarithm Do?\nWe use \\(log_2\\) because it assumes \\(S=2\\). - If we use some other logarithm, it‚Äôs no longer a bit! - Using a logarithm of the natural number \\(e\\) would produce ‚Äúnats‚Äù\n\nThe logarithm to the base 2 of a number is the power to which 2 must be raised to equal the number\nWe can express the same thing mathematically:\n\n\\[\n2^{\\log_2 x} = x\\\\\n\\]\n\nWith our three tosses of heads and tails:\n\n\\[\n\\log_2 8 = 3 \\\\\n2^{\\log_2 8} = 8 \\\\\n2^3 = 8\n\\]\nTwo must be raised to \\(3\\) to give us 8.\nPROMPT: What are the logarithms of the following numbers: 1, 2, 4, 8, 16, 32, 64?\n\n\n\n\\(\\mathbf{x}\\)\n\\(\\mathbf{\\log_2 x}\\)\nwhy?\n\n\n\n\n\\(1\\)\n\\(0\\)\n\\(2^0 = 1\\)\n\n\n\\(2\\)\n\\(1\\)\n\\(2^1 = 2\\)\n\n\n\\(4\\)\n\\(2\\)\n\\(2^2 = 4\\)\n\n\n\\(8\\)\n\\(3\\)\n\\(2^3 = 8\\)\n\n\n\\(16\\)\n\\(4\\)\n\\(2^4 = 16\\)\n\n\n\\(32\\)\n\\(5\\)\n\\(2^5 = 32\\)\n\n\n\\(64\\)\n\\(6\\)\n\\(2^6 = 64\\)\n\n\n\n\n\nCode Toggle\nplot_log()",
    "crumbs": [
      "Part I ‚Äî Information Theory",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Bits: The Difference that Makes a Difference</span>"
    ]
  }
]