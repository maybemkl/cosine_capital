This repo contains a comprehensive introduction to information theory for social scientists and other people who do not necessarily have a mathematical background, with a particular emphasis on the role of information theory in language modelling. The material is best accessed through the Quarto rendered tutorial at [mikaelbrunila.com/information-theory/](https://mikaelbrunila.com/information-theory/), but can also be used by cloning the repo and using the notebooks locally. 

This material also functions as an appendix to a number of my academic articles on large language models (LLMs) and information theory, including the articles ["Cosine Capital: Large Language Models and the Embedding of All Things"](https://journals.sagepub.com/doi/10.1177/20539517251386055) and ["Taking AI Into the Tunnels"](https://www.e-flux.com/journal/151/652643/taking-ai-into-the-tunnels/).

To date (November 18th 2025), I have only finished the first part that introduces the idea of language as a probability distribution and bits as a representation of these probabilities, "differences which make a difference.". I am working on completing the other parts on information theory, followed by a section on the relationship information theory and, respectively, embeddings (using word2vec) and attention (using GPT2-).